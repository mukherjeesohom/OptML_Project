{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OPTML.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a10897a9c3af41e0904881011d85b276": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48c42dd92b6a4597a677efdf0be2e84a",
              "IPY_MODEL_603abb200d7d4befa04accdc6035253b",
              "IPY_MODEL_8534bd3ac97a4258acadc6a6f4f7103c"
            ],
            "layout": "IPY_MODEL_4b38a28479234d368efba6ac7e11146a"
          }
        },
        "48c42dd92b6a4597a677efdf0be2e84a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcfe744ad59c40798abc6ecba4b13224",
            "placeholder": "​",
            "style": "IPY_MODEL_4a294002808c4652bd1a6422535ef2f1",
            "value": ""
          }
        },
        "603abb200d7d4befa04accdc6035253b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f8be2375184468fb8fdd1cf2eaef168",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ed0b0d035d74f9fb728648a38b820b9",
            "value": 170498071
          }
        },
        "8534bd3ac97a4258acadc6a6f4f7103c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6bf8fceab97467ca505a76fdc4cf4ff",
            "placeholder": "​",
            "style": "IPY_MODEL_9508406ae82d4acfb7dea8be8fb0e5b2",
            "value": " 170499072/? [00:13&lt;00:00, 15156435.87it/s]"
          }
        },
        "4b38a28479234d368efba6ac7e11146a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcfe744ad59c40798abc6ecba4b13224": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a294002808c4652bd1a6422535ef2f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f8be2375184468fb8fdd1cf2eaef168": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ed0b0d035d74f9fb728648a38b820b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6bf8fceab97467ca505a76fdc4cf4ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9508406ae82d4acfb7dea8be8fb0e5b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aPzqnIz-T95u"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.parallel\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#set variables\n",
        "\n",
        "#training options\n",
        "batch_size = 128\n",
        "lr_val = 0.1\n",
        "start_epoch = 1\n",
        "lr_decay = 0.1\n",
        "optimizer_val = 'sgd' #adam \n",
        "weight_decay = 0.0005\n",
        "momentum = 0.9\n",
        "epochs = 300\n",
        "ngpu = 1\n",
        "rand_seed = 0\n",
        "resume_model = ''\n",
        "resume_opt = ''\n",
        "\n",
        "#model parameters\n",
        "model = 'resnet18'\n",
        "loss_name = 'crossentropy'\n",
        "raw_data = False\n",
        "noaug = False\n",
        "label_corrupt_prob = 0.0\n",
        "trainloader_val = ''\n",
        "testloader_val = ''\n",
        "idx = 0\n",
        "\n",
        "datatype = 'cipher10'"
      ],
      "metadata": {
        "id": "Pr17sTxJUDYO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "# print('Current devices: ' + str(torch.cuda.current_device()))\n",
        "# print('Device count: ' + str(torch.cuda.device_count()))"
      ],
      "metadata": {
        "id": "pMcTERNYUHCU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_loaders(trainloader, testloader, datatype):\n",
        "    if trainloader and testloader:\n",
        "        assert os.path.exists(trainloader), 'trainloader does not exist'\n",
        "        assert os.path.exists(testloader), 'testloader does not exist'\n",
        "        trainloader = torch.load(trainloader)\n",
        "        testloader = torch.load(testloader)\n",
        "        return trainloader, testloader\n",
        "\n",
        "    normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
        "                                     std=[x/255.0 for x in [63.0, 62.1, 66.7]])\n",
        "\n",
        "    if raw_data:\n",
        "        transform_train = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "    else:\n",
        "        if not noaug:\n",
        "            # with data augmentation\n",
        "            transform_train = transforms.Compose([\n",
        "                transforms.RandomCrop(32, padding=4),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                normalize,\n",
        "            ])\n",
        "        else:\n",
        "            # no data agumentation\n",
        "            transform_train = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                normalize,\n",
        "            ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "\n",
        "\n",
        "    kwargs = {'num_workers': 2, 'pin_memory': True} if ngpu else {}\n",
        "\n",
        "    if datatype=='cipher10':\n",
        "      trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True,\n",
        "                                              transform=transform_train)\n",
        "      testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True,\n",
        "                                            transform=transform_test)\n",
        "\n",
        "      trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                                shuffle=True, **kwargs)\n",
        "      testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                              shuffle=False, **kwargs)\n",
        "    else:\n",
        "      trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True,\n",
        "                                              transform=transform_train)\n",
        "      testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True,\n",
        "                                            transform=transform_test)\n",
        "\n",
        "      trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                                shuffle=True, **kwargs)\n",
        "      testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                              shuffle=False, **kwargs)\n",
        "\n",
        "    return trainloader, testloader"
      ],
      "metadata": {
        "id": "ATjEXsU-YllX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#basicblock and resnet class\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1   = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2   = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=100):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1  = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1    = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ImageNet models\n",
        "def ResNet18():\n",
        "  if datatype=='cipher10':\n",
        "    return ResNet(BasicBlock, [2,2,2,2], 10)\n",
        "  \n",
        "  return ResNet(BasicBlock, [2,2,2,2], 100)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vh3saaDpa__G"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cfg = {\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name, num):\n",
        "        super(VGG, self).__init__()\n",
        "        self.input_size = 32\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.n_maps = cfg[vgg_name][-2]\n",
        "        self.fc = self._make_fc_layers()\n",
        "        self.classifier = nn.Linear(self.n_maps, num)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_fc_layers(self):\n",
        "        layers = []\n",
        "        layers += [nn.Linear(self.n_maps*self.input_size*self.input_size, self.n_maps),\n",
        "                   nn.BatchNorm1d(self.n_maps),\n",
        "                   nn.ReLU(inplace=True)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n",
        "                self.input_size = self.input_size // 2\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def VGG16():\n",
        "  if datatype=='cipher10':\n",
        "    return VGG('VGG16', 10)\n",
        "  return VGG('VGG16', 100)\n",
        "\n"
      ],
      "metadata": {
        "id": "zRg93FvGwQJv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# map between model name and function\n",
        "models = {\n",
        "    'resnet18'              : ResNet18,\n",
        "    'vgg16'                  : VGG16,\n",
        "}\n"
      ],
      "metadata": {
        "id": "x_HKn81FbSF-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load(model_name, model_file=None, data_parallel=False):\n",
        "    net = models[model_name]()\n",
        "    if data_parallel: # the model is saved in data paralle mode\n",
        "        net = torch.nn.DataParallel(net)\n",
        "\n",
        "    if model_file:\n",
        "        assert os.path.exists(model_file), model_file + \" does not exist.\"\n",
        "        stored = torch.load(model_file, map_location=lambda storage, loc: storage)\n",
        "        if 'state_dict' in stored.keys():\n",
        "            net.load_state_dict(stored['state_dict'])\n",
        "        else:\n",
        "            net.load_state_dict(stored)\n",
        "\n",
        "    if data_parallel: # convert the model back to the single GPU version\n",
        "        net = net.module\n",
        "\n",
        "    net.eval()\n",
        "    return net"
      ],
      "metadata": {
        "id": "ERa0lTljbWgh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize params; only for first training\n",
        "\n",
        "def init_params(net):\n",
        "    for m in net.modules():\n",
        "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "            init.kaiming_normal_(m.weight, mode='fan_in')\n",
        "            if m.bias is not None:\n",
        "                init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            init.constant_(m.weight, 1)\n",
        "            init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            init.normal_(m.weight, std=1e-3)\n",
        "            if m.bias is not None:\n",
        "                init.constant_(m.bias, 0)"
      ],
      "metadata": {
        "id": "5m1EOEMfbk3a"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "def train(trainloader, net, criterion, optimizer, use_cuda=True):\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    if isinstance(criterion, nn.CrossEntropyLoss):\n",
        "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "            batch_size = inputs.size(0)\n",
        "            total += batch_size\n",
        "            if use_cuda:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            inputs, targets = Variable(inputs), Variable(targets)\n",
        "            \n",
        "            \n",
        "            if optimizer_val=='sgd' or optimizer_val=='adam':\n",
        "              outputs = net(inputs)\n",
        "              loss = criterion(outputs, targets)\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "              \n",
        "           \n",
        "\n",
        "            train_loss += loss.item()*batch_size\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct += predicted.eq(targets.data).cpu().sum().item()\n",
        "\n",
        "    \n",
        "\n",
        "    return train_loss/total, 100 - 100.*correct/total,  100.*correct/total\n"
      ],
      "metadata": {
        "id": "roR8NY8rdmeT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(testloader, net, criterion, use_cuda=True):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    if isinstance(criterion, nn.CrossEntropyLoss):\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            batch_size = inputs.size(0)\n",
        "            total += batch_size\n",
        "\n",
        "            if use_cuda:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            inputs, targets = Variable(inputs), Variable(targets)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            test_loss += loss.item()*batch_size\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct += predicted.eq(targets.data).cpu().sum().item()\n",
        "\n",
        "  \n",
        "\n",
        "    return test_loss/total, 100 - 100.*correct/total,  100.*correct/total"
      ],
      "metadata": {
        "id": "qbywkJM8doQX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(lr_val, start_epoch):\n",
        "  random.seed(rand_seed)\n",
        "  np.random.seed(rand_seed)\n",
        "  torch.manual_seed(rand_seed)\n",
        "\n",
        "  if use_cuda:\n",
        "    torch.cuda.manual_seed_all(rand_seed)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "  \n",
        "  \n",
        "\n",
        "  trainloader, testloader = get_data_loaders(trainloader_val, testloader_val, datatype)\n",
        "\n",
        "\n",
        "\n",
        "  # Model\n",
        "  if resume_model:\n",
        "      # Load checkpoint.\n",
        "      print('==> Resuming from checkpoint..')\n",
        "      checkpoint = torch.load(resume_model)\n",
        "      net = load(model)\n",
        "     \n",
        "\n",
        "      net.load_state_dict(checkpoint['state_dict'])\n",
        "      start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "  else:\n",
        "      net = load(model)    \n",
        "\n",
        "  \n",
        "  if ngpu > 1:\n",
        "        net = torch.nn.DataParallel(net)\n",
        "\n",
        "  \n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "  if use_cuda:\n",
        "        net.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "\n",
        "  \n",
        "\n",
        "  # Optimizer\n",
        "  if optimizer_val == 'sgd':\n",
        "      optimizer = optim.SGD(net.parameters(), lr=lr_val, momentum=momentum, weight_decay=weight_decay, nesterov=True)\n",
        "  elif optimizer_val=='adam':\n",
        "      optimizer = optim.Adam(net.parameters(), lr=lr_val, weight_decay=weight_decay)\n",
        "  \n",
        "\n",
        "\n",
        "  \n",
        "  if resume_opt:\n",
        "    checkpoint_opt = torch.load(resume_opt)\n",
        "    optimizer.load_state_dict(checkpoint_opt['optimizer'])\n",
        "\n",
        "  \n",
        "\n",
        "  # record the performance of initial model\n",
        "  if not resume_model:\n",
        "      train_loss, train_err, train_acc = test(trainloader, net, criterion, use_cuda)\n",
        "      test_loss, test_err, test_acc = test(testloader, net, criterion, use_cuda)\n",
        "      status = 'e: %d loss: %.5f train_err: %.3f train_acc: %.3f test_top1: %.3f test_loss %.5f test_acc: %.3f\\n' % (0, train_loss, train_err, train_acc, test_err, test_loss, test_acc)\n",
        "      print(status)\n",
        "   \n",
        "  EPOCH_LIST = []\n",
        "  TRAIN_ACC_LIST = []\n",
        "  TEST_ACC_LIST = []\n",
        "  for epoch in range(start_epoch, epochs + 1):\n",
        "        EPOCH_LIST.append(epoch)\n",
        "        loss, train_err, train_acc = train(trainloader, net, criterion, optimizer, use_cuda)\n",
        "        TRAIN_ACC_LIST.append(train_acc)\n",
        "        test_loss, test_err, test_acc = test(testloader, net, criterion, use_cuda)\n",
        "        TEST_ACC_LIST.append(test_acc)\n",
        "        status = 'e: %d loss: %.5f train_err: %.3f train_acc: %.3f test_top1: %.3f test_loss %.5f test_acc: %.3f\\n' % (epoch, loss, train_err, train_acc, test_err, test_loss, test_acc)\n",
        "        print(status)\n",
        "       \n",
        "\n",
        "        if int(epoch) == 150 or int(epoch) == 225 or int(epoch) == 275:\n",
        "            lr_val *= lr_decay\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] *= lr_decay\n",
        "\n",
        "  torch.save(net.state_dict(),'model_'+model+\"_\"+datatype+\"_\"+optimizer_val+\"_\" '.pt')\n",
        "\n",
        "  return EPOCH_LIST, TRAIN_ACC_LIST, TEST_ACC_LIST\n",
        "\n"
      ],
      "metadata": {
        "id": "G1UiKXuUUJy7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(EPOCH_LIST, TRAIN_ACC_LIST, TEST_ACC_LIST):\n",
        "  plt.plot(EPOCH_LIST, TRAIN_ACC_LIST, label='train')\n",
        "  plt.plot(EPOCH_LIST, TEST_ACC_LIST, label='test')\n",
        "  plt.xlabel(\"epoch\")\n",
        "  plt.ylabel(\"accuracy\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "409gSGCgcz5B"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCH_LIST, TRAIN_ACC_LIST, TEST_ACC_LIST = main(lr_val, start_epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfAWcZYEUqG2",
        "outputId": "bf48b477-1270-4691-eb27-04b7a8003bca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "e: 0 loss: 2.30271 train_err: 89.748 train_acc: 10.252 test_top1: 89.390 test_loss 2.30278 test_acc: 10.610\n",
            "\n",
            "e: 1 loss: 1.95361 train_err: 68.602 train_acc: 31.398 test_top1: 58.430 test_loss 1.56835 test_acc: 41.570\n",
            "\n",
            "e: 2 loss: 1.47909 train_err: 54.330 train_acc: 45.670 test_top1: 50.450 test_loss 1.42110 test_acc: 49.550\n",
            "\n",
            "e: 3 loss: 1.19206 train_err: 42.722 train_acc: 57.278 test_top1: 40.750 test_loss 1.16173 test_acc: 59.250\n",
            "\n",
            "e: 4 loss: 0.97332 train_err: 34.474 train_acc: 65.526 test_top1: 44.520 test_loss 1.42416 test_acc: 55.480\n",
            "\n",
            "e: 5 loss: 0.83036 train_err: 29.452 train_acc: 70.548 test_top1: 27.620 test_loss 0.79036 test_acc: 72.380\n",
            "\n",
            "e: 6 loss: 0.70387 train_err: 24.652 train_acc: 75.348 test_top1: 25.650 test_loss 0.76711 test_acc: 74.350\n",
            "\n",
            "e: 7 loss: 0.61184 train_err: 21.316 train_acc: 78.684 test_top1: 21.980 test_loss 0.63028 test_acc: 78.020\n",
            "\n",
            "e: 8 loss: 0.55393 train_err: 19.168 train_acc: 80.832 test_top1: 25.910 test_loss 0.76743 test_acc: 74.090\n",
            "\n",
            "e: 9 loss: 0.51841 train_err: 17.850 train_acc: 82.150 test_top1: 19.850 test_loss 0.58900 test_acc: 80.150\n",
            "\n",
            "e: 10 loss: 0.49570 train_err: 17.022 train_acc: 82.978 test_top1: 25.940 test_loss 0.80407 test_acc: 74.060\n",
            "\n",
            "e: 11 loss: 0.47715 train_err: 16.338 train_acc: 83.662 test_top1: 25.450 test_loss 0.85456 test_acc: 74.550\n",
            "\n",
            "e: 12 loss: 0.45595 train_err: 15.684 train_acc: 84.316 test_top1: 21.280 test_loss 0.63689 test_acc: 78.720\n",
            "\n",
            "e: 13 loss: 0.44585 train_err: 15.386 train_acc: 84.614 test_top1: 17.940 test_loss 0.55379 test_acc: 82.060\n",
            "\n",
            "e: 14 loss: 0.42507 train_err: 14.698 train_acc: 85.302 test_top1: 21.140 test_loss 0.65395 test_acc: 78.860\n",
            "\n",
            "e: 15 loss: 0.41910 train_err: 14.342 train_acc: 85.658 test_top1: 21.440 test_loss 0.63487 test_acc: 78.560\n",
            "\n",
            "e: 16 loss: 0.40756 train_err: 14.040 train_acc: 85.960 test_top1: 16.340 test_loss 0.48524 test_acc: 83.660\n",
            "\n",
            "e: 17 loss: 0.40046 train_err: 13.824 train_acc: 86.176 test_top1: 19.680 test_loss 0.60120 test_acc: 80.320\n",
            "\n",
            "e: 18 loss: 0.39296 train_err: 13.440 train_acc: 86.560 test_top1: 24.820 test_loss 0.77407 test_acc: 75.180\n",
            "\n",
            "e: 19 loss: 0.38675 train_err: 13.280 train_acc: 86.720 test_top1: 19.530 test_loss 0.60983 test_acc: 80.470\n",
            "\n",
            "e: 20 loss: 0.38421 train_err: 13.076 train_acc: 86.924 test_top1: 16.650 test_loss 0.49320 test_acc: 83.350\n",
            "\n",
            "e: 21 loss: 0.37638 train_err: 12.922 train_acc: 87.078 test_top1: 17.670 test_loss 0.53496 test_acc: 82.330\n",
            "\n",
            "e: 22 loss: 0.36622 train_err: 12.586 train_acc: 87.414 test_top1: 18.930 test_loss 0.57150 test_acc: 81.070\n",
            "\n",
            "e: 23 loss: 0.36641 train_err: 12.474 train_acc: 87.526 test_top1: 16.670 test_loss 0.50171 test_acc: 83.330\n",
            "\n",
            "e: 24 loss: 0.36653 train_err: 12.584 train_acc: 87.416 test_top1: 21.320 test_loss 0.65460 test_acc: 78.680\n",
            "\n",
            "e: 25 loss: 0.35529 train_err: 12.294 train_acc: 87.706 test_top1: 19.440 test_loss 0.59834 test_acc: 80.560\n",
            "\n",
            "e: 26 loss: 0.35010 train_err: 11.942 train_acc: 88.058 test_top1: 15.970 test_loss 0.48062 test_acc: 84.030\n",
            "\n",
            "e: 27 loss: 0.35182 train_err: 12.048 train_acc: 87.952 test_top1: 16.200 test_loss 0.47439 test_acc: 83.800\n",
            "\n",
            "e: 28 loss: 0.34865 train_err: 11.918 train_acc: 88.082 test_top1: 17.510 test_loss 0.52558 test_acc: 82.490\n",
            "\n",
            "e: 29 loss: 0.34168 train_err: 11.830 train_acc: 88.170 test_top1: 15.450 test_loss 0.47008 test_acc: 84.550\n",
            "\n",
            "e: 30 loss: 0.34736 train_err: 11.750 train_acc: 88.250 test_top1: 18.330 test_loss 0.59784 test_acc: 81.670\n",
            "\n",
            "e: 31 loss: 0.33799 train_err: 11.732 train_acc: 88.268 test_top1: 18.410 test_loss 0.55056 test_acc: 81.590\n",
            "\n",
            "e: 32 loss: 0.33776 train_err: 11.418 train_acc: 88.582 test_top1: 17.360 test_loss 0.51903 test_acc: 82.640\n",
            "\n",
            "e: 33 loss: 0.33862 train_err: 11.522 train_acc: 88.478 test_top1: 18.210 test_loss 0.57619 test_acc: 81.790\n",
            "\n",
            "e: 34 loss: 0.33043 train_err: 11.412 train_acc: 88.588 test_top1: 18.450 test_loss 0.57923 test_acc: 81.550\n",
            "\n",
            "e: 35 loss: 0.33912 train_err: 11.580 train_acc: 88.420 test_top1: 22.880 test_loss 0.75024 test_acc: 77.120\n",
            "\n",
            "e: 36 loss: 0.33055 train_err: 11.436 train_acc: 88.564 test_top1: 17.400 test_loss 0.53828 test_acc: 82.600\n",
            "\n",
            "e: 37 loss: 0.33116 train_err: 11.318 train_acc: 88.682 test_top1: 17.550 test_loss 0.53151 test_acc: 82.450\n",
            "\n",
            "e: 38 loss: 0.33126 train_err: 11.296 train_acc: 88.704 test_top1: 17.680 test_loss 0.54478 test_acc: 82.320\n",
            "\n",
            "e: 39 loss: 0.32577 train_err: 11.070 train_acc: 88.930 test_top1: 16.890 test_loss 0.53446 test_acc: 83.110\n",
            "\n",
            "e: 40 loss: 0.32773 train_err: 11.146 train_acc: 88.854 test_top1: 16.480 test_loss 0.52180 test_acc: 83.520\n",
            "\n",
            "e: 41 loss: 0.32326 train_err: 10.940 train_acc: 89.060 test_top1: 14.900 test_loss 0.45968 test_acc: 85.100\n",
            "\n",
            "e: 42 loss: 0.32356 train_err: 10.988 train_acc: 89.012 test_top1: 16.410 test_loss 0.51129 test_acc: 83.590\n",
            "\n",
            "e: 43 loss: 0.32123 train_err: 10.936 train_acc: 89.064 test_top1: 14.730 test_loss 0.45078 test_acc: 85.270\n",
            "\n",
            "e: 44 loss: 0.32432 train_err: 11.138 train_acc: 88.862 test_top1: 19.660 test_loss 0.61292 test_acc: 80.340\n",
            "\n",
            "e: 45 loss: 0.31929 train_err: 10.884 train_acc: 89.116 test_top1: 14.490 test_loss 0.43652 test_acc: 85.510\n",
            "\n",
            "e: 46 loss: 0.32025 train_err: 11.016 train_acc: 88.984 test_top1: 16.510 test_loss 0.50201 test_acc: 83.490\n",
            "\n",
            "e: 47 loss: 0.31676 train_err: 10.806 train_acc: 89.194 test_top1: 16.540 test_loss 0.52243 test_acc: 83.460\n",
            "\n",
            "e: 48 loss: 0.31656 train_err: 10.756 train_acc: 89.244 test_top1: 13.950 test_loss 0.41955 test_acc: 86.050\n",
            "\n",
            "e: 49 loss: 0.31821 train_err: 10.820 train_acc: 89.180 test_top1: 17.870 test_loss 0.54787 test_acc: 82.130\n",
            "\n",
            "e: 50 loss: 0.31441 train_err: 10.814 train_acc: 89.186 test_top1: 14.730 test_loss 0.43999 test_acc: 85.270\n",
            "\n",
            "e: 51 loss: 0.31579 train_err: 10.774 train_acc: 89.226 test_top1: 16.630 test_loss 0.50743 test_acc: 83.370\n",
            "\n",
            "e: 52 loss: 0.31094 train_err: 10.548 train_acc: 89.452 test_top1: 16.860 test_loss 0.51499 test_acc: 83.140\n",
            "\n",
            "e: 53 loss: 0.31363 train_err: 10.560 train_acc: 89.440 test_top1: 16.800 test_loss 0.50698 test_acc: 83.200\n",
            "\n",
            "e: 54 loss: 0.31208 train_err: 10.606 train_acc: 89.394 test_top1: 16.450 test_loss 0.51403 test_acc: 83.550\n",
            "\n",
            "e: 55 loss: 0.31149 train_err: 10.580 train_acc: 89.420 test_top1: 20.580 test_loss 0.67382 test_acc: 79.420\n",
            "\n",
            "e: 56 loss: 0.31335 train_err: 10.666 train_acc: 89.334 test_top1: 17.110 test_loss 0.52289 test_acc: 82.890\n",
            "\n",
            "e: 57 loss: 0.30769 train_err: 10.526 train_acc: 89.474 test_top1: 14.250 test_loss 0.43947 test_acc: 85.750\n",
            "\n",
            "e: 58 loss: 0.31163 train_err: 10.656 train_acc: 89.344 test_top1: 15.350 test_loss 0.47491 test_acc: 84.650\n",
            "\n",
            "e: 59 loss: 0.30952 train_err: 10.512 train_acc: 89.488 test_top1: 14.590 test_loss 0.45730 test_acc: 85.410\n",
            "\n",
            "e: 60 loss: 0.30892 train_err: 10.600 train_acc: 89.400 test_top1: 19.440 test_loss 0.64097 test_acc: 80.560\n",
            "\n",
            "e: 61 loss: 0.31305 train_err: 10.692 train_acc: 89.308 test_top1: 14.500 test_loss 0.44654 test_acc: 85.500\n",
            "\n",
            "e: 62 loss: 0.30744 train_err: 10.538 train_acc: 89.462 test_top1: 16.700 test_loss 0.50658 test_acc: 83.300\n",
            "\n",
            "e: 63 loss: 0.30304 train_err: 10.416 train_acc: 89.584 test_top1: 15.290 test_loss 0.47597 test_acc: 84.710\n",
            "\n",
            "e: 64 loss: 0.30722 train_err: 10.376 train_acc: 89.624 test_top1: 16.800 test_loss 0.51375 test_acc: 83.200\n",
            "\n",
            "e: 65 loss: 0.30419 train_err: 10.376 train_acc: 89.624 test_top1: 15.590 test_loss 0.49762 test_acc: 84.410\n",
            "\n",
            "e: 66 loss: 0.30763 train_err: 10.516 train_acc: 89.484 test_top1: 15.480 test_loss 0.46396 test_acc: 84.520\n",
            "\n",
            "e: 67 loss: 0.30282 train_err: 10.240 train_acc: 89.760 test_top1: 13.740 test_loss 0.40427 test_acc: 86.260\n",
            "\n",
            "e: 68 loss: 0.30518 train_err: 10.400 train_acc: 89.600 test_top1: 14.520 test_loss 0.44131 test_acc: 85.480\n",
            "\n",
            "e: 69 loss: 0.30320 train_err: 10.252 train_acc: 89.748 test_top1: 14.190 test_loss 0.42410 test_acc: 85.810\n",
            "\n",
            "e: 70 loss: 0.30608 train_err: 10.486 train_acc: 89.514 test_top1: 14.340 test_loss 0.43903 test_acc: 85.660\n",
            "\n",
            "e: 71 loss: 0.29970 train_err: 10.284 train_acc: 89.716 test_top1: 13.670 test_loss 0.41287 test_acc: 86.330\n",
            "\n",
            "e: 72 loss: 0.30314 train_err: 10.328 train_acc: 89.672 test_top1: 18.060 test_loss 0.58977 test_acc: 81.940\n",
            "\n",
            "e: 73 loss: 0.30869 train_err: 10.718 train_acc: 89.282 test_top1: 14.640 test_loss 0.44842 test_acc: 85.360\n",
            "\n",
            "e: 74 loss: 0.30132 train_err: 10.370 train_acc: 89.630 test_top1: 14.630 test_loss 0.43157 test_acc: 85.370\n",
            "\n",
            "e: 75 loss: 0.30712 train_err: 10.566 train_acc: 89.434 test_top1: 18.260 test_loss 0.58007 test_acc: 81.740\n",
            "\n",
            "e: 76 loss: 0.30080 train_err: 10.356 train_acc: 89.644 test_top1: 15.250 test_loss 0.48847 test_acc: 84.750\n",
            "\n",
            "e: 77 loss: 0.30063 train_err: 10.236 train_acc: 89.764 test_top1: 13.050 test_loss 0.39098 test_acc: 86.950\n",
            "\n",
            "e: 78 loss: 0.30271 train_err: 10.276 train_acc: 89.724 test_top1: 13.520 test_loss 0.40554 test_acc: 86.480\n",
            "\n",
            "e: 79 loss: 0.29395 train_err: 9.912 train_acc: 90.088 test_top1: 13.000 test_loss 0.38343 test_acc: 87.000\n",
            "\n",
            "e: 80 loss: 0.30564 train_err: 10.364 train_acc: 89.636 test_top1: 12.260 test_loss 0.37681 test_acc: 87.740\n",
            "\n",
            "e: 81 loss: 0.30744 train_err: 10.486 train_acc: 89.514 test_top1: 13.360 test_loss 0.39940 test_acc: 86.640\n",
            "\n",
            "e: 82 loss: 0.30221 train_err: 10.134 train_acc: 89.866 test_top1: 15.890 test_loss 0.49429 test_acc: 84.110\n",
            "\n",
            "e: 83 loss: 0.29794 train_err: 10.192 train_acc: 89.808 test_top1: 17.240 test_loss 0.51592 test_acc: 82.760\n",
            "\n",
            "e: 84 loss: 0.29687 train_err: 9.976 train_acc: 90.024 test_top1: 17.850 test_loss 0.55463 test_acc: 82.150\n",
            "\n",
            "e: 85 loss: 0.30655 train_err: 10.284 train_acc: 89.716 test_top1: 13.050 test_loss 0.40080 test_acc: 86.950\n",
            "\n",
            "e: 86 loss: 0.29851 train_err: 10.142 train_acc: 89.858 test_top1: 14.480 test_loss 0.44967 test_acc: 85.520\n",
            "\n",
            "e: 87 loss: 0.29906 train_err: 10.228 train_acc: 89.772 test_top1: 15.150 test_loss 0.46217 test_acc: 84.850\n",
            "\n",
            "e: 88 loss: 0.30225 train_err: 10.390 train_acc: 89.610 test_top1: 18.070 test_loss 0.55933 test_acc: 81.930\n",
            "\n",
            "e: 89 loss: 0.29545 train_err: 9.984 train_acc: 90.016 test_top1: 14.200 test_loss 0.42476 test_acc: 85.800\n",
            "\n",
            "e: 90 loss: 0.29823 train_err: 10.164 train_acc: 89.836 test_top1: 16.570 test_loss 0.52378 test_acc: 83.430\n",
            "\n",
            "e: 91 loss: 0.30298 train_err: 10.370 train_acc: 89.630 test_top1: 17.070 test_loss 0.53436 test_acc: 82.930\n",
            "\n",
            "e: 92 loss: 0.29833 train_err: 10.200 train_acc: 89.800 test_top1: 13.700 test_loss 0.40172 test_acc: 86.300\n",
            "\n",
            "e: 93 loss: 0.29070 train_err: 9.764 train_acc: 90.236 test_top1: 15.210 test_loss 0.45583 test_acc: 84.790\n",
            "\n",
            "e: 94 loss: 0.30105 train_err: 10.214 train_acc: 89.786 test_top1: 14.530 test_loss 0.43795 test_acc: 85.470\n",
            "\n",
            "e: 95 loss: 0.30096 train_err: 10.222 train_acc: 89.778 test_top1: 16.570 test_loss 0.51448 test_acc: 83.430\n",
            "\n",
            "e: 96 loss: 0.29981 train_err: 10.286 train_acc: 89.714 test_top1: 16.730 test_loss 0.50715 test_acc: 83.270\n",
            "\n",
            "e: 97 loss: 0.29671 train_err: 10.056 train_acc: 89.944 test_top1: 13.290 test_loss 0.38998 test_acc: 86.710\n",
            "\n",
            "e: 98 loss: 0.29654 train_err: 10.008 train_acc: 89.992 test_top1: 16.310 test_loss 0.50863 test_acc: 83.690\n",
            "\n",
            "e: 99 loss: 0.29538 train_err: 10.052 train_acc: 89.948 test_top1: 13.650 test_loss 0.41300 test_acc: 86.350\n",
            "\n",
            "e: 100 loss: 0.29788 train_err: 10.028 train_acc: 89.972 test_top1: 16.050 test_loss 0.48810 test_acc: 83.950\n",
            "\n",
            "e: 101 loss: 0.29639 train_err: 10.084 train_acc: 89.916 test_top1: 24.520 test_loss 0.86048 test_acc: 75.480\n",
            "\n",
            "e: 102 loss: 0.29572 train_err: 10.214 train_acc: 89.786 test_top1: 14.700 test_loss 0.44503 test_acc: 85.300\n",
            "\n",
            "e: 103 loss: 0.29625 train_err: 10.012 train_acc: 89.988 test_top1: 14.780 test_loss 0.45517 test_acc: 85.220\n",
            "\n",
            "e: 104 loss: 0.29751 train_err: 10.162 train_acc: 89.838 test_top1: 16.390 test_loss 0.49029 test_acc: 83.610\n",
            "\n",
            "e: 105 loss: 0.29926 train_err: 10.180 train_acc: 89.820 test_top1: 16.160 test_loss 0.49719 test_acc: 83.840\n",
            "\n",
            "e: 106 loss: 0.29617 train_err: 10.154 train_acc: 89.846 test_top1: 17.720 test_loss 0.55857 test_acc: 82.280\n",
            "\n",
            "e: 107 loss: 0.29443 train_err: 9.962 train_acc: 90.038 test_top1: 14.730 test_loss 0.44264 test_acc: 85.270\n",
            "\n",
            "e: 108 loss: 0.29859 train_err: 10.234 train_acc: 89.766 test_top1: 15.890 test_loss 0.49864 test_acc: 84.110\n",
            "\n",
            "e: 109 loss: 0.29484 train_err: 10.048 train_acc: 89.952 test_top1: 13.670 test_loss 0.42169 test_acc: 86.330\n",
            "\n",
            "e: 110 loss: 0.29370 train_err: 9.950 train_acc: 90.050 test_top1: 16.850 test_loss 0.53413 test_acc: 83.150\n",
            "\n",
            "e: 111 loss: 0.29836 train_err: 10.172 train_acc: 89.828 test_top1: 15.390 test_loss 0.45905 test_acc: 84.610\n",
            "\n",
            "e: 112 loss: 0.29384 train_err: 9.930 train_acc: 90.070 test_top1: 17.770 test_loss 0.56632 test_acc: 82.230\n",
            "\n",
            "e: 113 loss: 0.29576 train_err: 10.168 train_acc: 89.832 test_top1: 14.390 test_loss 0.42822 test_acc: 85.610\n",
            "\n",
            "e: 114 loss: 0.29694 train_err: 10.200 train_acc: 89.800 test_top1: 17.830 test_loss 0.54973 test_acc: 82.170\n",
            "\n",
            "e: 115 loss: 0.29108 train_err: 9.918 train_acc: 90.082 test_top1: 13.530 test_loss 0.39768 test_acc: 86.470\n",
            "\n",
            "e: 116 loss: 0.29662 train_err: 10.054 train_acc: 89.946 test_top1: 16.970 test_loss 0.53241 test_acc: 83.030\n",
            "\n",
            "e: 117 loss: 0.29601 train_err: 10.110 train_acc: 89.890 test_top1: 13.980 test_loss 0.43194 test_acc: 86.020\n",
            "\n",
            "e: 118 loss: 0.29041 train_err: 9.872 train_acc: 90.128 test_top1: 17.320 test_loss 0.54762 test_acc: 82.680\n",
            "\n",
            "e: 119 loss: 0.29155 train_err: 9.954 train_acc: 90.046 test_top1: 15.410 test_loss 0.45017 test_acc: 84.590\n",
            "\n",
            "e: 120 loss: 0.29514 train_err: 10.028 train_acc: 89.972 test_top1: 20.320 test_loss 0.65319 test_acc: 79.680\n",
            "\n",
            "e: 121 loss: 0.29179 train_err: 9.810 train_acc: 90.190 test_top1: 13.560 test_loss 0.40442 test_acc: 86.440\n",
            "\n",
            "e: 122 loss: 0.29518 train_err: 10.148 train_acc: 89.852 test_top1: 17.840 test_loss 0.49266 test_acc: 82.160\n",
            "\n",
            "e: 123 loss: 0.29119 train_err: 9.916 train_acc: 90.084 test_top1: 15.130 test_loss 0.47445 test_acc: 84.870\n",
            "\n",
            "e: 124 loss: 0.29701 train_err: 10.002 train_acc: 89.998 test_top1: 15.050 test_loss 0.44709 test_acc: 84.950\n",
            "\n",
            "e: 125 loss: 0.29688 train_err: 10.058 train_acc: 89.942 test_top1: 16.630 test_loss 0.50161 test_acc: 83.370\n",
            "\n",
            "e: 126 loss: 0.28758 train_err: 9.762 train_acc: 90.238 test_top1: 14.930 test_loss 0.44851 test_acc: 85.070\n",
            "\n",
            "e: 127 loss: 0.29051 train_err: 9.822 train_acc: 90.178 test_top1: 12.560 test_loss 0.38237 test_acc: 87.440\n",
            "\n",
            "e: 128 loss: 0.29334 train_err: 9.966 train_acc: 90.034 test_top1: 14.950 test_loss 0.45156 test_acc: 85.050\n",
            "\n",
            "e: 129 loss: 0.29722 train_err: 10.030 train_acc: 89.970 test_top1: 14.760 test_loss 0.44825 test_acc: 85.240\n",
            "\n",
            "e: 130 loss: 0.29046 train_err: 9.920 train_acc: 90.080 test_top1: 16.730 test_loss 0.51639 test_acc: 83.270\n",
            "\n",
            "e: 131 loss: 0.29439 train_err: 10.046 train_acc: 89.954 test_top1: 21.360 test_loss 0.69582 test_acc: 78.640\n",
            "\n",
            "e: 132 loss: 0.29373 train_err: 10.046 train_acc: 89.954 test_top1: 19.660 test_loss 0.60461 test_acc: 80.340\n",
            "\n",
            "e: 133 loss: 0.29170 train_err: 10.038 train_acc: 89.962 test_top1: 12.970 test_loss 0.40630 test_acc: 87.030\n",
            "\n",
            "e: 134 loss: 0.29192 train_err: 9.848 train_acc: 90.152 test_top1: 14.020 test_loss 0.41145 test_acc: 85.980\n",
            "\n",
            "e: 135 loss: 0.29309 train_err: 9.870 train_acc: 90.130 test_top1: 13.900 test_loss 0.42798 test_acc: 86.100\n",
            "\n",
            "e: 136 loss: 0.29491 train_err: 10.044 train_acc: 89.956 test_top1: 23.870 test_loss 0.74857 test_acc: 76.130\n",
            "\n",
            "e: 137 loss: 0.28924 train_err: 9.944 train_acc: 90.056 test_top1: 15.850 test_loss 0.47971 test_acc: 84.150\n",
            "\n",
            "e: 138 loss: 0.28925 train_err: 9.824 train_acc: 90.176 test_top1: 13.240 test_loss 0.40724 test_acc: 86.760\n",
            "\n",
            "e: 139 loss: 0.29305 train_err: 9.896 train_acc: 90.104 test_top1: 16.430 test_loss 0.52663 test_acc: 83.570\n",
            "\n",
            "e: 140 loss: 0.28677 train_err: 9.772 train_acc: 90.228 test_top1: 14.760 test_loss 0.44819 test_acc: 85.240\n",
            "\n",
            "e: 141 loss: 0.29271 train_err: 9.852 train_acc: 90.148 test_top1: 15.610 test_loss 0.48756 test_acc: 84.390\n",
            "\n",
            "e: 142 loss: 0.29502 train_err: 9.986 train_acc: 90.014 test_top1: 13.530 test_loss 0.40379 test_acc: 86.470\n",
            "\n",
            "e: 143 loss: 0.29022 train_err: 9.878 train_acc: 90.122 test_top1: 13.380 test_loss 0.42147 test_acc: 86.620\n",
            "\n",
            "e: 144 loss: 0.29338 train_err: 9.884 train_acc: 90.116 test_top1: 20.200 test_loss 0.66333 test_acc: 79.800\n",
            "\n",
            "e: 145 loss: 0.29309 train_err: 9.950 train_acc: 90.050 test_top1: 13.060 test_loss 0.40676 test_acc: 86.940\n",
            "\n",
            "e: 146 loss: 0.29364 train_err: 9.858 train_acc: 90.142 test_top1: 14.150 test_loss 0.41287 test_acc: 85.850\n",
            "\n",
            "e: 147 loss: 0.28895 train_err: 9.984 train_acc: 90.016 test_top1: 12.150 test_loss 0.35304 test_acc: 87.850\n",
            "\n",
            "e: 148 loss: 0.29563 train_err: 9.962 train_acc: 90.038 test_top1: 19.610 test_loss 0.58821 test_acc: 80.390\n",
            "\n",
            "e: 149 loss: 0.29622 train_err: 10.068 train_acc: 89.932 test_top1: 12.970 test_loss 0.38724 test_acc: 87.030\n",
            "\n",
            "e: 150 loss: 0.28792 train_err: 9.716 train_acc: 90.284 test_top1: 21.470 test_loss 0.69571 test_acc: 78.530\n",
            "\n",
            "e: 151 loss: 0.15044 train_err: 4.984 train_acc: 95.016 test_top1: 6.590 test_loss 0.19913 test_acc: 93.410\n",
            "\n",
            "e: 152 loss: 0.10270 train_err: 3.348 train_acc: 96.652 test_top1: 6.220 test_loss 0.18889 test_acc: 93.780\n",
            "\n",
            "e: 153 loss: 0.08673 train_err: 2.840 train_acc: 97.160 test_top1: 6.060 test_loss 0.18912 test_acc: 93.940\n",
            "\n",
            "e: 154 loss: 0.07284 train_err: 2.396 train_acc: 97.604 test_top1: 5.820 test_loss 0.18292 test_acc: 94.180\n",
            "\n",
            "e: 155 loss: 0.06680 train_err: 2.202 train_acc: 97.798 test_top1: 6.070 test_loss 0.19244 test_acc: 93.930\n",
            "\n",
            "e: 156 loss: 0.05924 train_err: 1.906 train_acc: 98.094 test_top1: 5.950 test_loss 0.18710 test_acc: 94.050\n",
            "\n",
            "e: 157 loss: 0.05232 train_err: 1.644 train_acc: 98.356 test_top1: 5.850 test_loss 0.19312 test_acc: 94.150\n",
            "\n",
            "e: 158 loss: 0.04651 train_err: 1.478 train_acc: 98.522 test_top1: 5.750 test_loss 0.19716 test_acc: 94.250\n",
            "\n",
            "e: 159 loss: 0.04124 train_err: 1.338 train_acc: 98.662 test_top1: 5.840 test_loss 0.20469 test_acc: 94.160\n",
            "\n",
            "e: 160 loss: 0.03932 train_err: 1.272 train_acc: 98.728 test_top1: 6.180 test_loss 0.21293 test_acc: 93.820\n",
            "\n",
            "e: 161 loss: 0.03443 train_err: 1.118 train_acc: 98.882 test_top1: 6.100 test_loss 0.21229 test_acc: 93.900\n",
            "\n",
            "e: 162 loss: 0.03422 train_err: 1.084 train_acc: 98.916 test_top1: 5.820 test_loss 0.20915 test_acc: 94.180\n",
            "\n",
            "e: 163 loss: 0.03172 train_err: 1.026 train_acc: 98.974 test_top1: 6.210 test_loss 0.22575 test_acc: 93.790\n",
            "\n",
            "e: 164 loss: 0.03313 train_err: 1.064 train_acc: 98.936 test_top1: 5.930 test_loss 0.21375 test_acc: 94.070\n",
            "\n",
            "e: 165 loss: 0.03068 train_err: 1.012 train_acc: 98.988 test_top1: 5.830 test_loss 0.21186 test_acc: 94.170\n",
            "\n",
            "e: 166 loss: 0.02747 train_err: 0.870 train_acc: 99.130 test_top1: 5.840 test_loss 0.21485 test_acc: 94.160\n",
            "\n",
            "e: 167 loss: 0.02841 train_err: 0.896 train_acc: 99.104 test_top1: 5.920 test_loss 0.22529 test_acc: 94.080\n",
            "\n",
            "e: 168 loss: 0.02720 train_err: 0.874 train_acc: 99.126 test_top1: 6.240 test_loss 0.22870 test_acc: 93.760\n",
            "\n",
            "e: 169 loss: 0.02729 train_err: 0.884 train_acc: 99.116 test_top1: 6.020 test_loss 0.22701 test_acc: 93.980\n",
            "\n",
            "e: 170 loss: 0.02790 train_err: 0.900 train_acc: 99.100 test_top1: 6.180 test_loss 0.23077 test_acc: 93.820\n",
            "\n",
            "e: 171 loss: 0.03174 train_err: 1.078 train_acc: 98.922 test_top1: 6.040 test_loss 0.22588 test_acc: 93.960\n",
            "\n",
            "e: 172 loss: 0.02732 train_err: 0.874 train_acc: 99.126 test_top1: 6.540 test_loss 0.24748 test_acc: 93.460\n",
            "\n",
            "e: 173 loss: 0.03041 train_err: 1.038 train_acc: 98.962 test_top1: 6.320 test_loss 0.23798 test_acc: 93.680\n",
            "\n",
            "e: 174 loss: 0.02900 train_err: 0.928 train_acc: 99.072 test_top1: 6.500 test_loss 0.23832 test_acc: 93.500\n",
            "\n",
            "e: 175 loss: 0.02987 train_err: 0.968 train_acc: 99.032 test_top1: 6.200 test_loss 0.21993 test_acc: 93.800\n",
            "\n",
            "e: 176 loss: 0.02901 train_err: 0.954 train_acc: 99.046 test_top1: 6.460 test_loss 0.23929 test_acc: 93.540\n",
            "\n",
            "e: 177 loss: 0.03274 train_err: 1.060 train_acc: 98.940 test_top1: 6.430 test_loss 0.23070 test_acc: 93.570\n",
            "\n",
            "e: 178 loss: 0.03159 train_err: 1.042 train_acc: 98.958 test_top1: 6.170 test_loss 0.23145 test_acc: 93.830\n",
            "\n",
            "e: 179 loss: 0.03077 train_err: 0.968 train_acc: 99.032 test_top1: 6.800 test_loss 0.24677 test_acc: 93.200\n",
            "\n",
            "e: 180 loss: 0.03699 train_err: 1.204 train_acc: 98.796 test_top1: 6.740 test_loss 0.25090 test_acc: 93.260\n",
            "\n",
            "e: 181 loss: 0.03223 train_err: 1.052 train_acc: 98.948 test_top1: 6.000 test_loss 0.22183 test_acc: 94.000\n",
            "\n",
            "e: 182 loss: 0.03823 train_err: 1.236 train_acc: 98.764 test_top1: 6.520 test_loss 0.23206 test_acc: 93.480\n",
            "\n",
            "e: 183 loss: 0.03661 train_err: 1.230 train_acc: 98.770 test_top1: 7.010 test_loss 0.25431 test_acc: 92.990\n",
            "\n",
            "e: 184 loss: 0.03698 train_err: 1.200 train_acc: 98.800 test_top1: 6.390 test_loss 0.23214 test_acc: 93.610\n",
            "\n",
            "e: 185 loss: 0.03384 train_err: 1.094 train_acc: 98.906 test_top1: 6.570 test_loss 0.24153 test_acc: 93.430\n",
            "\n",
            "e: 186 loss: 0.04059 train_err: 1.338 train_acc: 98.662 test_top1: 7.160 test_loss 0.26439 test_acc: 92.840\n",
            "\n",
            "e: 187 loss: 0.03843 train_err: 1.234 train_acc: 98.766 test_top1: 7.310 test_loss 0.26694 test_acc: 92.690\n",
            "\n",
            "e: 188 loss: 0.03861 train_err: 1.250 train_acc: 98.750 test_top1: 6.870 test_loss 0.24704 test_acc: 93.130\n",
            "\n",
            "e: 189 loss: 0.04439 train_err: 1.466 train_acc: 98.534 test_top1: 7.140 test_loss 0.25573 test_acc: 92.860\n",
            "\n",
            "e: 190 loss: 0.03945 train_err: 1.284 train_acc: 98.716 test_top1: 6.420 test_loss 0.22604 test_acc: 93.580\n",
            "\n",
            "e: 191 loss: 0.04173 train_err: 1.384 train_acc: 98.616 test_top1: 6.350 test_loss 0.25247 test_acc: 93.650\n",
            "\n",
            "e: 192 loss: 0.04208 train_err: 1.342 train_acc: 98.658 test_top1: 6.600 test_loss 0.24057 test_acc: 93.400\n",
            "\n",
            "e: 193 loss: 0.03618 train_err: 1.160 train_acc: 98.840 test_top1: 6.530 test_loss 0.24421 test_acc: 93.470\n",
            "\n",
            "e: 194 loss: 0.04011 train_err: 1.332 train_acc: 98.668 test_top1: 6.600 test_loss 0.24250 test_acc: 93.400\n",
            "\n",
            "e: 195 loss: 0.04423 train_err: 1.460 train_acc: 98.540 test_top1: 6.810 test_loss 0.25028 test_acc: 93.190\n",
            "\n",
            "e: 196 loss: 0.04227 train_err: 1.402 train_acc: 98.598 test_top1: 7.310 test_loss 0.26070 test_acc: 92.690\n",
            "\n",
            "e: 197 loss: 0.04616 train_err: 1.524 train_acc: 98.476 test_top1: 6.990 test_loss 0.26132 test_acc: 93.010\n",
            "\n",
            "e: 198 loss: 0.04701 train_err: 1.560 train_acc: 98.440 test_top1: 7.760 test_loss 0.28196 test_acc: 92.240\n",
            "\n",
            "e: 199 loss: 0.04087 train_err: 1.288 train_acc: 98.712 test_top1: 6.660 test_loss 0.24929 test_acc: 93.340\n",
            "\n",
            "e: 200 loss: 0.04933 train_err: 1.646 train_acc: 98.354 test_top1: 8.040 test_loss 0.29512 test_acc: 91.960\n",
            "\n",
            "e: 201 loss: 0.04652 train_err: 1.522 train_acc: 98.478 test_top1: 6.760 test_loss 0.25141 test_acc: 93.240\n",
            "\n",
            "e: 202 loss: 0.04847 train_err: 1.596 train_acc: 98.404 test_top1: 7.210 test_loss 0.25065 test_acc: 92.790\n",
            "\n",
            "e: 203 loss: 0.04507 train_err: 1.418 train_acc: 98.582 test_top1: 7.300 test_loss 0.26732 test_acc: 92.700\n",
            "\n",
            "e: 204 loss: 0.04828 train_err: 1.582 train_acc: 98.418 test_top1: 7.490 test_loss 0.26365 test_acc: 92.510\n",
            "\n",
            "e: 205 loss: 0.04315 train_err: 1.418 train_acc: 98.582 test_top1: 7.460 test_loss 0.27555 test_acc: 92.540\n",
            "\n",
            "e: 206 loss: 0.04383 train_err: 1.396 train_acc: 98.604 test_top1: 7.410 test_loss 0.27144 test_acc: 92.590\n",
            "\n",
            "e: 207 loss: 0.04405 train_err: 1.390 train_acc: 98.610 test_top1: 7.710 test_loss 0.28227 test_acc: 92.290\n",
            "\n",
            "e: 208 loss: 0.04269 train_err: 1.380 train_acc: 98.620 test_top1: 7.440 test_loss 0.26858 test_acc: 92.560\n",
            "\n",
            "e: 209 loss: 0.04652 train_err: 1.524 train_acc: 98.476 test_top1: 7.510 test_loss 0.27129 test_acc: 92.490\n",
            "\n",
            "e: 210 loss: 0.04540 train_err: 1.550 train_acc: 98.450 test_top1: 7.550 test_loss 0.27212 test_acc: 92.450\n",
            "\n",
            "e: 211 loss: 0.04376 train_err: 1.456 train_acc: 98.544 test_top1: 7.640 test_loss 0.27216 test_acc: 92.360\n",
            "\n",
            "e: 212 loss: 0.04303 train_err: 1.366 train_acc: 98.634 test_top1: 7.380 test_loss 0.26820 test_acc: 92.620\n",
            "\n",
            "e: 213 loss: 0.04159 train_err: 1.326 train_acc: 98.674 test_top1: 6.800 test_loss 0.25679 test_acc: 93.200\n",
            "\n",
            "e: 214 loss: 0.04282 train_err: 1.414 train_acc: 98.586 test_top1: 6.860 test_loss 0.26345 test_acc: 93.140\n",
            "\n",
            "e: 215 loss: 0.04493 train_err: 1.430 train_acc: 98.570 test_top1: 6.880 test_loss 0.25942 test_acc: 93.120\n",
            "\n",
            "e: 216 loss: 0.04726 train_err: 1.554 train_acc: 98.446 test_top1: 7.540 test_loss 0.26368 test_acc: 92.460\n",
            "\n",
            "e: 217 loss: 0.04630 train_err: 1.538 train_acc: 98.462 test_top1: 7.450 test_loss 0.26806 test_acc: 92.550\n",
            "\n",
            "e: 218 loss: 0.04050 train_err: 1.352 train_acc: 98.648 test_top1: 7.120 test_loss 0.25433 test_acc: 92.880\n",
            "\n",
            "e: 219 loss: 0.04781 train_err: 1.622 train_acc: 98.378 test_top1: 7.360 test_loss 0.27812 test_acc: 92.640\n",
            "\n",
            "e: 220 loss: 0.04173 train_err: 1.334 train_acc: 98.666 test_top1: 7.390 test_loss 0.27654 test_acc: 92.610\n",
            "\n",
            "e: 221 loss: 0.04400 train_err: 1.448 train_acc: 98.552 test_top1: 7.480 test_loss 0.27226 test_acc: 92.520\n",
            "\n",
            "e: 222 loss: 0.04424 train_err: 1.432 train_acc: 98.568 test_top1: 7.950 test_loss 0.28869 test_acc: 92.050\n",
            "\n",
            "e: 223 loss: 0.05006 train_err: 1.606 train_acc: 98.394 test_top1: 8.120 test_loss 0.27902 test_acc: 91.880\n",
            "\n",
            "e: 224 loss: 0.04647 train_err: 1.572 train_acc: 98.428 test_top1: 7.890 test_loss 0.28334 test_acc: 92.110\n",
            "\n",
            "e: 225 loss: 0.04208 train_err: 1.396 train_acc: 98.604 test_top1: 6.910 test_loss 0.25192 test_acc: 93.090\n",
            "\n",
            "e: 226 loss: 0.02027 train_err: 0.548 train_acc: 99.452 test_top1: 5.580 test_loss 0.20950 test_acc: 94.420\n",
            "\n",
            "e: 227 loss: 0.01079 train_err: 0.254 train_acc: 99.746 test_top1: 5.530 test_loss 0.20463 test_acc: 94.470\n",
            "\n",
            "e: 228 loss: 0.00799 train_err: 0.162 train_acc: 99.838 test_top1: 5.430 test_loss 0.20586 test_acc: 94.570\n",
            "\n",
            "e: 229 loss: 0.00740 train_err: 0.144 train_acc: 99.856 test_top1: 5.280 test_loss 0.20354 test_acc: 94.720\n",
            "\n",
            "e: 230 loss: 0.00579 train_err: 0.106 train_acc: 99.894 test_top1: 5.210 test_loss 0.20168 test_acc: 94.790\n",
            "\n",
            "e: 231 loss: 0.00528 train_err: 0.082 train_acc: 99.918 test_top1: 5.130 test_loss 0.20168 test_acc: 94.870\n",
            "\n",
            "e: 232 loss: 0.00517 train_err: 0.092 train_acc: 99.908 test_top1: 5.120 test_loss 0.20092 test_acc: 94.880\n",
            "\n",
            "e: 233 loss: 0.00472 train_err: 0.068 train_acc: 99.932 test_top1: 5.010 test_loss 0.20130 test_acc: 94.990\n",
            "\n",
            "e: 234 loss: 0.00451 train_err: 0.084 train_acc: 99.916 test_top1: 5.040 test_loss 0.20003 test_acc: 94.960\n",
            "\n",
            "e: 235 loss: 0.00455 train_err: 0.052 train_acc: 99.948 test_top1: 5.070 test_loss 0.19994 test_acc: 94.930\n",
            "\n",
            "e: 236 loss: 0.00401 train_err: 0.056 train_acc: 99.944 test_top1: 5.130 test_loss 0.20114 test_acc: 94.870\n",
            "\n",
            "e: 237 loss: 0.00371 train_err: 0.058 train_acc: 99.942 test_top1: 5.090 test_loss 0.20059 test_acc: 94.910\n",
            "\n",
            "e: 238 loss: 0.00359 train_err: 0.048 train_acc: 99.952 test_top1: 5.000 test_loss 0.19961 test_acc: 95.000\n",
            "\n",
            "e: 239 loss: 0.00351 train_err: 0.062 train_acc: 99.938 test_top1: 5.040 test_loss 0.20078 test_acc: 94.960\n",
            "\n",
            "e: 240 loss: 0.00314 train_err: 0.038 train_acc: 99.962 test_top1: 5.050 test_loss 0.20041 test_acc: 94.950\n",
            "\n",
            "e: 241 loss: 0.00334 train_err: 0.048 train_acc: 99.952 test_top1: 5.020 test_loss 0.20019 test_acc: 94.980\n",
            "\n",
            "e: 242 loss: 0.00278 train_err: 0.022 train_acc: 99.978 test_top1: 5.090 test_loss 0.19956 test_acc: 94.910\n",
            "\n",
            "e: 243 loss: 0.00271 train_err: 0.016 train_acc: 99.984 test_top1: 5.040 test_loss 0.19805 test_acc: 94.960\n",
            "\n",
            "e: 244 loss: 0.00299 train_err: 0.032 train_acc: 99.968 test_top1: 5.100 test_loss 0.20126 test_acc: 94.900\n",
            "\n",
            "e: 245 loss: 0.00273 train_err: 0.016 train_acc: 99.984 test_top1: 5.000 test_loss 0.19879 test_acc: 95.000\n",
            "\n",
            "e: 246 loss: 0.00247 train_err: 0.016 train_acc: 99.984 test_top1: 4.980 test_loss 0.19902 test_acc: 95.020\n",
            "\n",
            "e: 247 loss: 0.00260 train_err: 0.014 train_acc: 99.986 test_top1: 5.030 test_loss 0.19999 test_acc: 94.970\n",
            "\n",
            "e: 248 loss: 0.00245 train_err: 0.014 train_acc: 99.986 test_top1: 5.090 test_loss 0.19948 test_acc: 94.910\n",
            "\n",
            "e: 249 loss: 0.00272 train_err: 0.016 train_acc: 99.984 test_top1: 5.070 test_loss 0.19805 test_acc: 94.930\n",
            "\n",
            "e: 250 loss: 0.00255 train_err: 0.022 train_acc: 99.978 test_top1: 4.920 test_loss 0.20032 test_acc: 95.080\n",
            "\n",
            "e: 251 loss: 0.00252 train_err: 0.026 train_acc: 99.974 test_top1: 4.900 test_loss 0.19698 test_acc: 95.100\n",
            "\n",
            "e: 252 loss: 0.00235 train_err: 0.016 train_acc: 99.984 test_top1: 4.880 test_loss 0.19725 test_acc: 95.120\n",
            "\n",
            "e: 253 loss: 0.00258 train_err: 0.028 train_acc: 99.972 test_top1: 4.840 test_loss 0.19534 test_acc: 95.160\n",
            "\n",
            "e: 254 loss: 0.00260 train_err: 0.026 train_acc: 99.974 test_top1: 5.040 test_loss 0.19601 test_acc: 94.960\n",
            "\n",
            "e: 255 loss: 0.00245 train_err: 0.026 train_acc: 99.974 test_top1: 4.900 test_loss 0.19605 test_acc: 95.100\n",
            "\n",
            "e: 256 loss: 0.00213 train_err: 0.006 train_acc: 99.994 test_top1: 4.920 test_loss 0.19709 test_acc: 95.080\n",
            "\n",
            "e: 257 loss: 0.00211 train_err: 0.008 train_acc: 99.992 test_top1: 4.900 test_loss 0.19303 test_acc: 95.100\n",
            "\n",
            "e: 258 loss: 0.00241 train_err: 0.026 train_acc: 99.974 test_top1: 4.890 test_loss 0.19554 test_acc: 95.110\n",
            "\n",
            "e: 259 loss: 0.00209 train_err: 0.010 train_acc: 99.990 test_top1: 4.890 test_loss 0.19411 test_acc: 95.110\n",
            "\n",
            "e: 260 loss: 0.00227 train_err: 0.018 train_acc: 99.982 test_top1: 4.920 test_loss 0.19496 test_acc: 95.080\n",
            "\n",
            "e: 261 loss: 0.00207 train_err: 0.008 train_acc: 99.992 test_top1: 4.810 test_loss 0.19659 test_acc: 95.190\n",
            "\n",
            "e: 262 loss: 0.00206 train_err: 0.010 train_acc: 99.990 test_top1: 4.940 test_loss 0.19504 test_acc: 95.060\n",
            "\n",
            "e: 263 loss: 0.00207 train_err: 0.016 train_acc: 99.984 test_top1: 4.980 test_loss 0.19351 test_acc: 95.020\n",
            "\n",
            "e: 264 loss: 0.00209 train_err: 0.012 train_acc: 99.988 test_top1: 4.890 test_loss 0.19438 test_acc: 95.110\n",
            "\n",
            "e: 265 loss: 0.00209 train_err: 0.010 train_acc: 99.990 test_top1: 4.860 test_loss 0.19409 test_acc: 95.140\n",
            "\n",
            "e: 266 loss: 0.00195 train_err: 0.010 train_acc: 99.990 test_top1: 4.950 test_loss 0.19451 test_acc: 95.050\n",
            "\n",
            "e: 267 loss: 0.00190 train_err: 0.002 train_acc: 99.998 test_top1: 4.830 test_loss 0.19400 test_acc: 95.170\n",
            "\n",
            "e: 268 loss: 0.00207 train_err: 0.016 train_acc: 99.984 test_top1: 4.950 test_loss 0.19341 test_acc: 95.050\n",
            "\n",
            "e: 269 loss: 0.00197 train_err: 0.004 train_acc: 99.996 test_top1: 4.900 test_loss 0.19413 test_acc: 95.100\n",
            "\n",
            "e: 270 loss: 0.00206 train_err: 0.014 train_acc: 99.986 test_top1: 4.920 test_loss 0.19418 test_acc: 95.080\n",
            "\n",
            "e: 271 loss: 0.00196 train_err: 0.008 train_acc: 99.992 test_top1: 4.920 test_loss 0.19366 test_acc: 95.080\n",
            "\n",
            "e: 272 loss: 0.00199 train_err: 0.010 train_acc: 99.990 test_top1: 4.930 test_loss 0.19243 test_acc: 95.070\n",
            "\n",
            "e: 273 loss: 0.00197 train_err: 0.012 train_acc: 99.988 test_top1: 4.880 test_loss 0.19217 test_acc: 95.120\n",
            "\n",
            "e: 274 loss: 0.00205 train_err: 0.012 train_acc: 99.988 test_top1: 4.820 test_loss 0.19123 test_acc: 95.180\n",
            "\n",
            "e: 275 loss: 0.00190 train_err: 0.006 train_acc: 99.994 test_top1: 4.840 test_loss 0.19170 test_acc: 95.160\n",
            "\n",
            "e: 276 loss: 0.00190 train_err: 0.006 train_acc: 99.994 test_top1: 4.840 test_loss 0.19170 test_acc: 95.160\n",
            "\n",
            "e: 277 loss: 0.00187 train_err: 0.010 train_acc: 99.990 test_top1: 4.850 test_loss 0.19230 test_acc: 95.150\n",
            "\n",
            "e: 278 loss: 0.00181 train_err: 0.002 train_acc: 99.998 test_top1: 4.870 test_loss 0.19158 test_acc: 95.130\n",
            "\n",
            "e: 279 loss: 0.00191 train_err: 0.010 train_acc: 99.990 test_top1: 4.890 test_loss 0.19180 test_acc: 95.110\n",
            "\n",
            "e: 280 loss: 0.00193 train_err: 0.012 train_acc: 99.988 test_top1: 4.880 test_loss 0.19059 test_acc: 95.120\n",
            "\n",
            "e: 281 loss: 0.00184 train_err: 0.008 train_acc: 99.992 test_top1: 4.850 test_loss 0.19078 test_acc: 95.150\n",
            "\n",
            "e: 282 loss: 0.00186 train_err: 0.006 train_acc: 99.994 test_top1: 4.840 test_loss 0.19045 test_acc: 95.160\n",
            "\n",
            "e: 283 loss: 0.00189 train_err: 0.006 train_acc: 99.994 test_top1: 4.850 test_loss 0.19139 test_acc: 95.150\n",
            "\n",
            "e: 284 loss: 0.00186 train_err: 0.004 train_acc: 99.996 test_top1: 4.900 test_loss 0.19129 test_acc: 95.100\n",
            "\n",
            "e: 285 loss: 0.00190 train_err: 0.012 train_acc: 99.988 test_top1: 4.880 test_loss 0.19047 test_acc: 95.120\n",
            "\n",
            "e: 286 loss: 0.00184 train_err: 0.004 train_acc: 99.996 test_top1: 4.890 test_loss 0.19178 test_acc: 95.110\n",
            "\n",
            "e: 287 loss: 0.00185 train_err: 0.002 train_acc: 99.998 test_top1: 4.870 test_loss 0.19140 test_acc: 95.130\n",
            "\n",
            "e: 288 loss: 0.00189 train_err: 0.008 train_acc: 99.992 test_top1: 4.860 test_loss 0.19217 test_acc: 95.140\n",
            "\n",
            "e: 289 loss: 0.00193 train_err: 0.008 train_acc: 99.992 test_top1: 4.810 test_loss 0.19251 test_acc: 95.190\n",
            "\n",
            "e: 290 loss: 0.00191 train_err: 0.008 train_acc: 99.992 test_top1: 4.850 test_loss 0.19289 test_acc: 95.150\n",
            "\n",
            "e: 291 loss: 0.00164 train_err: 0.000 train_acc: 100.000 test_top1: 4.940 test_loss 0.19144 test_acc: 95.060\n",
            "\n",
            "e: 292 loss: 0.00182 train_err: 0.006 train_acc: 99.994 test_top1: 4.860 test_loss 0.19208 test_acc: 95.140\n",
            "\n",
            "e: 293 loss: 0.00196 train_err: 0.012 train_acc: 99.988 test_top1: 4.890 test_loss 0.19191 test_acc: 95.110\n",
            "\n",
            "e: 294 loss: 0.00193 train_err: 0.008 train_acc: 99.992 test_top1: 4.850 test_loss 0.19304 test_acc: 95.150\n",
            "\n",
            "e: 295 loss: 0.00192 train_err: 0.004 train_acc: 99.996 test_top1: 4.740 test_loss 0.19155 test_acc: 95.260\n",
            "\n",
            "e: 296 loss: 0.00195 train_err: 0.016 train_acc: 99.984 test_top1: 4.850 test_loss 0.19233 test_acc: 95.150\n",
            "\n",
            "e: 297 loss: 0.00170 train_err: 0.002 train_acc: 99.998 test_top1: 4.920 test_loss 0.19321 test_acc: 95.080\n",
            "\n",
            "e: 298 loss: 0.00181 train_err: 0.004 train_acc: 99.996 test_top1: 4.790 test_loss 0.19172 test_acc: 95.210\n",
            "\n",
            "e: 299 loss: 0.00181 train_err: 0.002 train_acc: 99.998 test_top1: 4.860 test_loss 0.19160 test_acc: 95.140\n",
            "\n",
            "e: 300 loss: 0.00180 train_err: 0.004 train_acc: 99.996 test_top1: 4.860 test_loss 0.19102 test_acc: 95.140\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot(EPOCH_LIST, TRAIN_ACC_LIST, TEST_ACC_LIST)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "Ywllto2bbqyw",
        "outputId": "e8836311-a342-4476-8897-f7a94784eb96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZhcVZn48e9bW1fvazr7TiBhDSGENUjYZF8UEQFlEI2OqOgoI44yyMyoOOMPER1BFkdc2AQRZBMCIYrIEiBAyJ6QpbN0dzq9V3et5/fHuberulOdVDrdXV2p9/M8/VTVrVt1z03Bee95z7nniDEGpZRSCsCT7QIopZQaOTQoKKWU6qFBQSmlVA8NCkoppXpoUFBKKdXDl+0C7I+amhozZcqUbBdDKaVyyltvvbXTGDMq3Xs5HRSmTJnC0qVLs10MpZTKKSKyqb/3NH2klFKqhwYFpZRSPTQoKKWU6pHTfQrpRKNR6urq6O7uznZRhlQwGGTChAn4/f5sF0UpdQA54IJCXV0dpaWlTJkyBRHJdnGGhDGGpqYm6urqmDp1araLo5Q6gAxZ+khEfiUiDSKyPGVblYi8ICJrncdKZ7uIyB0isk5E3hOROQM9bnd3N9XV1QdsQAAQEaqrqw/41pBSavgNZZ/Cr4Gz+2y7EXjRGDMDeNF5DXAOMMP5WwjcuT8HPpADgisfzlEpNfyGLH1kjPmriEzps/ki4FTn+f3Ay8C3nO2/MXYe79dEpEJExhpjtg9V+ZRSI1MoEiMUidMdtX9dkQTdsThd7rZYAmMMxoDBkEhAwhi6Ywm6IjESBvpbEcDQ/1IB+7qKwJ6WHej/+Pt+7P7KfNrMWo6cUNH/BwdouPsURqdU9DuA0c7z8cCWlP3qnG27BQURWYhtTTBp0qShK+kAtbS08MADD/ClL31pnz537rnn8sADD1BRMfg/slK54IUV9dz67ErWN3Zmuyg5oaak4IAICj2MMUZE9nmFH2PM3cDdAHPnzh1xKwS1tLTwi1/8YregEIvF8Pn6/+d+5plnhrpoSo1Yi1bUs/C3SzlkdCnfPOtgygr9BP1egn4vhX4vQb/HefRS4PPg8QgCeEQQAUEoDHgpDHjxOqnVgWRY+/uMPVrm+9vP9HeMfr5rAOUaCsMdFOrdtJCIjAUanO1bgYkp+01wtuWcG2+8kfXr1zN79mz8fj/BYJDKykpWrVrFmjVruPjii9myZQvd3d1cf/31LFy4EEhO2dHR0cE555zDySefzKuvvsr48eN54oknKCwszPKZqaHQ0N7NGx/uYvOuEJ3hGH6vh1fXNfHOlmaCfi+JhE0ejC4LcujYMt7e3MycSZVs2tVJWdCP1yPMnlhBOJZgR2s3lxw9nvq2bhbMrGVUSQG7QhFWbGtj7pRKigLp/3dv7YpSHPDi82bvtqU7l6xnclURf7ruJIJ+b9bKoYY/KDwJXA3c6jw+kbL9yyLyEHAc0DoY/Qm3/PkDVmxr29+v6eXQcWXcfMFh/b5/6623snz5cpYtW8bLL7/Meeedx/Lly3uGjv7qV7+iqqqKrq4ujj32WD7+8Y9TXV3d6zvWrl3Lgw8+yD333MNll13GY489xlVXXTWo56GypzUU5ZGlW3h5TQNvfLiLaNw2eD1ic87Taor5zAlTiCcMXo+9RFy2pYW/fLCDeVOreG1DE7PGltEVjdMZjvGzl9bh9wpBv5cn393W67vcXPW48iCReAIR4bwjxvLhzk5qSwvoCMf4ywc7mFBZxFXHTyLo9zKtpoQpNUX8v+fXMGdyJR+fMx6PCG9taqYw4KWjO8aa+nab74/GmVxVzIKZtTS0d/PquiZW7WjH7xVmjillUnURx0yuYsmaRrbsCnHI6FJeXd/EoePKqCr285GDa3njw128tamZ7543SwPCCDBkQUFEHsR2KteISB1wMzYYPCIi1wKbgMuc3Z8BzgXWASHgmqEq13CbN29er3sJ7rjjDh5//HEAtmzZwtq1a3cLClOnTmX27NkAHHPMMWzcuHHYyquGTjxh+NlLa/nlkg10ReMcOraMz5wwhYtnj2fqqGKK/F6iiQQFvt0rRrdj1ePZPY8QjsXxezx0RGIsWlHP5OoilqxuBKCs0M+o0gJ+//pmJlQU0h6O8etXNzK2PMi7dS0EfV6uOG4SK7a18YNnVu323Y+/s5X/+PMHxBImbWeoyO6dpGPKgoQiMR56c8vuH8AGrITzmZICHx3hGDUlAT5xzMS0+6vhNZSjjz7Vz1unp9nXANcNdhn2dEU/XIqLi3uev/zyyyxatIh//OMfFBUVceqpp6a916CgoKDnudfrpaura1jKqoZOSyjC9Q8tY8maRs47YixfWjCdw8aV77ZfgSf9lbI4ufN03CBSFvTzsTkTADhmclWvfS6aPb7neUNbN5XFAfx90kV1zSE8Iizf2sqmphAnTK/mj2/bLG5RwMvk6iJiCUNRwMupB9dSGPDi9wpvb25m+dY2SoM+Tpxew5jyIImEYVcowvt1raxtaGdydTHGGN6ra+WLp05nZ3uYVTvaeXX9TiZWFvHJYydSXqR3548EB9wdzdlWWlpKe3t72vdaW1uprKykqKiIVatW8dprrw1z6VQ2PPP+dm7603LauqP84JIj+NS8iVm9z6S2LJh2+4TKIgDGVST7rw4fv3vg6uuYyVW7BSGPR6gpKWDBzFoWzKzt2X724WMBG8CmjSrh3CPG7nP51dDSoDDIqqurOemkkzj88MMpLCxk9OjRPe+dffbZ3HXXXcyaNYtDDjmE448/PoslVcNh+dZWvvbQMmaNLeX7l8zLqJJVKptkTzdgjHRz5841fRfZWblyJbNmzcpSiYZXPp1rLuoIxzj/jr/RHU3wzPXzqSoOZLtISgEgIm8ZY+ame09bCkoNkR8+s5LNu0I8tPAEDQgqZ+h6CkoNAWMMz6+o5/wjxzFvatXeP6DUCKFBQakhsK21m8b2MHOnVGa7KErtEw0KSg2BZZtbAJg9UeeyUrlF+xSUGgLLtjQT8HmYOaYs20U5MERC4C/sPQlQVzP4gnZ7PApef+/3QrugZDQUlEAiAZtegV0fQqgJulthysnJfRGIhyEWtt+5cw34i2D6AuhogKa1UFBm3491Qc3B9ju8BeArgEin/UxoJ/gKbZn8RRAoAo/f+e6IfUSgdDQk4vb74mH7+Y4GW97aWdC6xX53y2aoe8Meb+xRsO0dKB4FBaVw8Edh/DGD/k+tQUGpIfD+1lYOG1dGwKeN8X0W7oC1z8PWt6BhJbRvh4YVtrItH28rxbattsIE8PggEYPiWlu5FlZCW13y+0rHQjxig4FLvPD32/svg8dvv/PlH2Rebo8Pimps0Ih22WPuRpzbwBO7f7a4FjobIRFNbvcFYdwcWPcivPew/f5wuw0kJaM1KOSCgU6dDXD77bezcOFCioqKhqBkajg1toc5ZExptouRO7qa4e3fwPZ3Ye0LEG6zV8q1s6BsHBx6ka0MW+ugc6etDOdeaytXd9/WLfZqvrMRRs20AaRtGzStB48Hpi2AicdBUbWthJc/BsEyexUO4A3Yq/5oKFn51i+3V+VjjoSOeluhewvs86JqW/HHwvZzlVN6t1biMftd8Sj4AraCdwNY5067r3tMb8B+d7Tbtkoqp9iWREGZLbsx9tzLxtvXQ0iDwiDrb+rsTNx+++1cddVVGhQOAC2hKBVFAVth/O3/2SvHwy6BdYtg9bM29XDYJbDg32DZ76H+A5i30FaMjasgWAHTT4MjPgGRdls5RLtsOmJvOhqhZNTQn+Rgev1ue1VeMgZmXQBHXwXj59rKdKjM7m8mHkewzAYWV0FJ8nnq9v54feBNkz70+qGsnzu5/UEYc8Tu20WgYnjmhtKgMMhSp84+88wzqa2t5ZFHHiEcDnPJJZdwyy230NnZyWWXXUZdXR3xeJybbrqJ+vp6tm3bxoIFC6ipqWHx4sXZPhU1QMYYWrqinBBaDD+/IpnmePUOQGDCXKiaCn/7Mbz7UDLV8cbdIB6oOQRCy+C9h2DR96B9G5RPsvnqzzzpXFn64Z3fwQnXQetWeOvXcOYtNuf8+0/AeT+GYz9n89jpKtZ41JarYrKtvLJtw2KbJlmo/91n2wj4r2EIPXsj7Hh/cL9zzBFwzq39vp06dfbzzz/Po48+yhtvvIExhgsvvJC//vWvNDY2Mm7cOJ5++mnAzolUXl7ObbfdxuLFi6mpqRncMqth1dYd41z+zgVrfw7jjoYLfmo7HXd9CAedDiXOXECv/9Lmzk+4zqYU1j4PF94BVdNsumDx920eed4XYNd62N4N953hHEUAYwNJPGqfe31Qv8I+/8t3beviiS/D1Plw0S9s5yZAZxP8cr7Ny489yqZPimth9hUw5vDkiTSth6Iq22qBoVvpJdwOdW/CiV8dmu9X++TADgpZ9vzzz/P8889z9NFHA9DR0cHatWuZP38+3/jGN/jWt77F+eefz/z587NcUjWYWkIRPuN7ntaS6ZRf85xNCQBM6jPX1XFfsH+uk1IqRRE47bv2z/Xh32DJj2DGmTbdNPsKWP2czW23bLKtBYBTboA374M/ft52um58BZ79V7jo5/D8TXaUS9s2+91v3Gvz26EmeP1OOO82GyjKJ8A9p8HM82wwKx8PH7sns8Dw0vdtvr5yMhx0hg16e7LpVRsUp5269+9WQ+7ADgp7uKIfDsYYvv3tb/OFL3xht/fefvttnnnmGb773e9y+umn8+///u9ZKKEaCrs6IxQQJVo6ORkQBsPU+fYv1bRT7WNXiw0O1dNh9pW2Yn/kM3DW920H5cs/gMbV0LjS7n/Yx2zwOOUG5/PN8Ohn4Zkb7OiX0nHQ3QIrn4Jwq90nEYO5n4XJJzkdrTU2jZWIJ1NQ296Bv/4PjD7cHvcv/2Y7hZc9YIPTgn+znbY1M5IBZuWTECixncAq6w7soJAFqVNnf/SjH+Wmm27iyiuvpKSkhK1bt+L3+4nFYlRVVXHVVVdRUVHBvffe2+uzmj7KbS2hKGOI4fUX7H3nwVJYYfsUXLMugBuc9E+027Yk3n8UzrgFPF44/NI+n6+ES+6Ge09zRro4C+S4AWHaAlj3EnzwuB3OaeJ2hIy/CJrW2VFCow+3KbCSWrjmabvfHbPh1+fZgFIyBh671n7fqJlwzbN2PP+KJ+3oosEMoGrANCgMstSps8855xyuuOIKTjjhBABKSkr43e9+x7p167jhhhvweDz4/X7uvPNOABYuXMjZZ5/NuHHjtKM5hzWHIkwihr9gGINCOkXOnEv+IFz8C7jwZzYg9KdkFHz1XZtKuv1wGH2YvVegdBx8+nE7kuqd30L7DtsqeeU2e1/AsZ+39xGsec6mnS75JQSdKcIv+CmseAJmng8zzrJDTuuXw3M3wqPXwIRj7ZDSIy/rv1xqWOnU2Tksn841l9z3yoec9fyZjD5iAYFP3JPt4gzMjvftTV+/Otve1Xvu/+y+T1ezHS0VHMAaEW/dD0//i21BHHQmXPHwngOWGlQ6dbZSw6glFMEvMfyBHE6HuGPlF75sb6xKp3A/Jvs75mobbJo3wpT5QzeySe0zDQpKDbLmUIQCiSFDeePVcEm9YWuwVUyyf2pEycrELCJyvYgsF5EPRORrzrYqEXlBRNY6jwO+DMnllFim8uEcc1VzKIqfeP9X2EqNYMMeFETkcODzwDzgKOB8ETkIuBF40RgzA3jReb3PgsEgTU1NB3SlaYyhqamJYDC30hPrGtqpaw4N+XHiCdPzuL6xo9d7sXiCRSvqeXtzc89+fbWEIuzqtJOZGWPYsivEzo4wr6zdybItLcTiCWLxBN3ROMYYuqNxYvHkBGfNnRH8xHrPg6NUjshG+mgW8LoxJgQgIkuAjwEXAac6+9wPvAx8a1+/fMKECdTV1dHY2DgohR2pgsEgEyZMGNZjGmNY39jJlx94m8+ePJXL5k5kTX07OzvCPPzmFk6ZMYpZY8soCniZWFXEj55bRUmBj2tPnsqW5hAf+8WrFBf4eOyLJzK+spAXVuzg5dWNxBKGooAXv9fD7IkVtHVH2drcxWkzawn4PDy7fAebm0Kcf+RYnnp/Ox3dMQ6qLWHpxl187YyDWbmjjSfe2caJB1XTGoryxLvbmFpTjM8jrNrRzlmHjmbljjbGVxQSjiV4x1nrYHxFIYeOK2NNfTuHjyunpiTAzs4Izy3fQTxhOHpSBa2hKBt2dvb6dxhVWkAoHKMzEqeiyE9Hd4yyQj+Tq4soL/Tzjw1N+Ati2lJQOWnYRx+JyCzgCeAEoAvbKlgKfNoYU+HsI0Cz+7rP5xcCCwEmTZp0zKZNm4ar6CNWImHY2RHG5/VQ4PMQ9HtJGENzKEJ5oZ8tu7qYWlOM1yMs39rK8yvqaWzvJuD1UF7o5/5/bOKoiRV8uLODUw+u5c2Nu6gsCvD1Mw/mpVUNHDmhnOeW7+CFFfVE4gniCYPfK3xq3iR++9omjHFmA075T2l0WQH1bWEAakoChGMJCnxeOsMxuqJxAl4PkXiCyiI/fq+HcMxeeYdj9oo79ftEwO/1EIklqCmxFe3OjghVxYGeK/qZY0pZ39hBPGG49JgJ7OyIsL21m/EVQRatbODYKZVE4obtLV185fQZlAV9/Pndbaxv7GRqTTEfbGulO5rA7/Vw7hFjGF0W5I9v1zGuopDTZ9YSjRum1BQTisR4/oN6ygr9TKgspK45RFmhn+0t3TR1hvmwsZMjx5dw14azYMF34CP/Onz/ISiVoT2NPsrKkFQRuRb4EtAJfACEgX9KDQIi0myM2WO/QrohqQeqeMIQiSXY0dbN4lUNROMJassKaOuK8djbdbxX19qzb4HPQ3GBj12dEYoDXjojcUqDPmpLC1jf2IlHoKq4gFAkRigS56gJ5TS2hxldHuSdzS0cMb6c7a3d7OwI93xnSYGPC44aS3HAxzlHjOE/nlrJu1taOOXgUXz6+MkcPr6MR5fWUVEcIJEwPPzmFuYfXMPZh43hZy+tozDg5V/OPJiuSJzXP9zF9pYuJlcXccVxk/F6pOcc39y4i0TCcNDoEpZvbSWRgGmjioknDO/WtXLBUWMJxxJsbgoxbVQxr6zdid/n4dSDRxFLGLqiccqCybSNMYYPd9qKX4ZrhEu0C74/Bs74Hpz89eE5plL7YMQFhV4FEPkBUAdcD5xqjNkuImOBl40xh+zpswdaUIjFEzz45hZ2tHbx93VNbGrqxABjyoJ0R+Nsa+nGYIjGe/9mNSUFfH7+VAp89op7R1s3TR0RZtSWsKU5xOyJlby/tYXNu0KcfdgYLpw9nvJCP12ROBt2dnDo2DJExMmfdzGxqpB1DR18+4/vc81JU/EIHDO5ktqy3n0Ym5tCjKsI4vPqQjK9dLfCrZPgoz/Y+7w/SmXBiLtPQURqjTENIjIJ259wPDAVuBq41Xl8IhtlG2qJhOH3r2+iKxqntSvK2PJCjDE8/s5W1tR30BGO4RGoLQ1y9uFjAOEf63cSiSW4fN5EvB7hc/OnURzwsqvTpocqigI9V9v9233oX2HAy2HjkjceiQiTqu18/TNGl/LoP5+452+s1nUf0oo7K2dpn4LKQdm6T+ExEakGosB1xpgWEbkVeMRJLW0Ccv6+93jCIEB9ezevbWhi6cZmVm5v422no9Mj4A6AmTmmlI/PGc8J02s49ZBR+DzScwWeSBjixuDvc0VeUaSVzojkLsOoo49UDspKUDDG7DZXtDGmCTg9C8UZEm9t2sW199vUVkvIXjmWFviYUFXEzRccyjmHj6WowMv6hg4KfF4OHdf/Au8ej+BhhN3x2dlkJ2HTqQmsTa/aO3xrZ6UEBQ3aKvfoHc2D6L26Fl5a1UDA5+HnL62jtrSAOZMrmVJdzBmzRnPImNLd0jxHT9qPqQKypbUOfn6snQb5xK9kuzQDs+0dGDt78KZX+L9z7OP3Wu1qZ6BBQeUkDQqDwBjDr1/dyC1/XtGz7agJ5dzzmbm7dc4eEF79mV2Q/P1HczMobH8P7j4VrngEDv5o7/cioczWQU4V2pV8Ho/afxvQoKBykgaF/WCM4aVVDdz67CrWNnRw5qGj+fGlR9HYEWZSVREB3wE0KicSgt9caBebf+t+KCiH7cucdX6dTuyHrrTTIx9zdWbfGe4Ak7ALpA+nBid413/QOyh0t8GtE+HUf4NTU+6b3P6eXTsgXTmfuQHatydfP3QldOywzzUoqBx0ANVaw2tbSxcX/PwVrr1/KfGE4bbLjuLOK+dQXuTnoNqS3A4I0e7dtzWssOvo/uXfbAfq5b+z21c/Zx8TcVj1FPx5H9bZ/dMX7Wpf0W47L//eJBLJkT2pQrvg3Yd23x6Pwaqne99VB7BzrX1sWt97e6dzF/zLP0hui3bDvWfA63clt733B/jZMXb5yzfuhpV/ttuLauyCM41r7GvtaFY5KIdrruxZU9/Op+97nU07Q/zo40fw3NdO4WNzJgzfeP2uZjvl8GBoWg93nZysINcugu+Phi1v9N6vcbV99BfZhVOmngKFVcnlHbtakvu6OfVU95xul4fs+50NK+HP18ODl++9rG/8En46e/dK/r2H4fEv2FZL27bk9nWL4KEr7MIuvc55Xe9HVyRlnqSl/2eDTcsmiIftOsVgA80T19nPbnq19+cLSuwi9LEu+1pbCioHafpoHz2xbCvf/MO7FBf4uPfquRw3rXr4C7Hoe/DhX+Gr7/S/T2iXXTVrxpl7/q51i+yCKou+Zxdaf+s3dvuO92DivOR+O1eDxw83bkmux1s1FXZtsM+7UvLq296BScfBB3+y3znuaNi61P6lat9hK+Ktb9nn7nwZ/dn0KrTV2ZXBilOWLO1osI+v/9L+ff0DKB2dLFNHfe/vcQPgrj4thXBKUHjqa/bO5Orp9nVbnfOZDTZI9N0fbGsndZsGBZWDtKWwD3772ia+9vAy5kyq5MV/+cjwBoQP/gSv/tw+b1hpr4oTif73f/Zb8PtLYcub9rPNG+Hpb+yeftnmBJaVT9oOZHdN3kgINiyxqZFY2KZEqqcnAwJA1bRkUEjtbP1wiX18+hvw1x/bz7teuR3W/MVWnuE226fQtBYi7TZ9k4jbllBfxiTTPm1b7eOKJ+H2I5M5/fWL7aLzzc5VvVtBd6ZMjphI2GDgDdjtqS2csF1bm9Nucv4NOpItBLcF4vZHeAuSfQfBciifZO9kdlsJoOkjlZO0pZChV9bu5KY/LeeMWbX8/Io5BP1DPD5/wxK7OPqS/7EV5o737fYTrrMVVSIGL94C61+CL/5t98+HmuzjfWfYxwnzoO4NOPgcmHFGcr9t78CYI6FkNJz8NfD47ELrm1+DF5zK8eP32ZaCuxqXq2oaLH/MpovcijxQAm/cA8d9wZahab1tBbgW3Wwfv/zW7mVuWgd/uw1ev9MuPP9Jp99i8Q9hya3JK++2bTD2KNvH0bLJHhOSFbZbgYfb7GPnThsMulvs1X80BNNPh/Uvwo8mw+cXw/g5yfTRoRfBy7fafd1/x9atcO+ZTtpOYPwx9tgAZ9xiWyMv/7D3+WhLQeUgbSlkwBjD/zy/mvEVhfzvlcMQEMCOann6G7D51WRAAFvJdjrpktXP2DRPR5ppwk2892t3mGTLRttyMMZeSTeuhpnnwVWPwpSTYdLxUDwKtr2d/OyuDbYyrOkzFVXlVHul37I5mao57zZbviX/DRj7WffKPlW6bU3roHWLfb7q6eSV+5Jb7aN7U1jLFjtSyA02O1e7J20f2rfbFodbyW95w1b+/z0Vlj1gtx2V0oex/kX76AaRQAn4CyHWnWwpxLpsUO1sgIqJNn3lHr+gFALFu5+PBgWVgzQo7EUiYfhPZ0bQr5x2EAW+AQQEY+wC6G//NrP9u5ptReemZlKtfjr53E2n1L+/+34tm/uUwUk1Lf4h/OFq28J48suAsTdxpSqq7j3Mcsvr9vPVB/Xer2qafdy1IZk+OvijUDUdVj9rX8fDyU7rmefDqFnJ7+yraZ29OnfLu9UJTDUH995v0c3w0yOTV+qJWO/3X/0Z/EeVTbOB7TdxK/zXfgEIHHKOvdGscoodmgrJdFNBKfiCNpA2b7R9Kanattl93MAbKO4nKGj6SOUeDQp78cjSLfzq7x/yTydO4RNzJ+7bh1c/Z6/EOxpg8z9s53Am6tKkVsT5qValBAX3ynhHn6CQSNi7jk/4MnzqYbvNDRKhnfbxqa/BB4/D3Gth+mm9P1+U0ldSPgnqnA7iij7n3xMU1tuWgnhtfr1iUu9O3I2v2McLfwYXOf0i65yrc18QEKiYbFtBsW4Yfbjz7/CmfXRbDGCnkoiGbODc1k9HuxvQdiy3j/GwPUb5JFvOmoNtpQ72WD1Bod3uFygGf9AGqJZNNlUE4Cu0KbTzbkt+HmzLIpBmLWNtKagcpEFhD7qjcW5ftJY5kyq4+YJD7RQVxtir4Hhszx+OhODBT8L/zoO/3263uamRval7wwYBX8rd0O6VfrorbLfyM8aOq++ot6mWqqkwxqlgI31GyrRshlEz4fzboO8C8+7InqIaGwi6nc7Y8gm771dca0cPhXbZCltk9/02/s12zBZW2mMCbHnNVqTVM6BsnN3evMlWxKVjoHwivPSf8MAnbZpm4nFw3BdtK8QVTzP0NZWbZgMoHWvTY2D7D1y1hzotlG77b1RQas/BV2jPKdZtR0+B7cf44iv25rzUIFBQoukjdcDQoLAHv/3HJna0dXPDR2cmF2ipX27H1K9+xr42Bpb+Kpk+WfW0fZ6afnntTvvYN6WTTrQLVj4FtYfBpBPS7xMosR3Crq1LbY791+fBbbOSaZWKyfZegr4KnOmyZ5yV/vvdlkLpWCiptc/FY1+nErH3K3z4V3sFXuQcqyJlmm5f0FasZWPt/gUltlxgK+DpC+yw2WCZ7VCPddvPuHcar3kOMDDnM3DOj5Itnb7cAFqQctdxatConAITj7XP3St/gNGH2YDbuMqmmNwWgD+Y7GSummrvz5h0XPJzBSlBIdBPUOgbbJXKARoU+tHeHeV/X17HKQeP4oTpKekUd0y8e9XfuBqe+jo8/Gmba37oCtuZ6XZCVk6lVwdoujtyUz3zTWj4wC7j+PH74MKfJ987+llUlswAAB7HSURBVCqYczWc+R+2Mxjg8EttTv/WibDp77ZydtNJFZNsh6m3wL4uKLeVmztfUd95f1xFTkuhdAyUjHGej02fI5/2Edsy2fyabQmAvcoH8BfDrAvt886Uytw9/vhj4Kz/tDfD+YuSI4P8hXDWf8HlDyQ/47Y+gs7ifP4+8xPVzkp+ZzqVk20QHHsUHHTG7p/bucb2KbgtAF9hMgAVlMHnXoRTbkh+LtA3KGj6SB0YdEhqPx58YzMtoSjfPKtPJ6c79NId9tjq3NS06RV7tQk2beG2FGZfAYu/b5+bhB1107TOXi3XzLDbG9fYK+RgmQ0ox18HhzqV6aiUET8zPprc/tav7TE+8q82nbFhiS3Lew8lO2jLJ9ir86Iqu+8Rl8LZP7RX/eNmw+ST0p98sRMEy8bam8Dc70pn6kfsY0c9jHPSMm7fQ8koe3X//iO2k9k17/Nw5GW2D8IVKLYpN8Re9fsL7bBRj9/ee+AGmst+Y1Nob95nU1ClY+25Hf5xWzFPnQ8bFu9ezorJ9hy+0Kdfx20Jde60fQq9WgpO6y9QBKMP7f251BaJpo/UAURbCmnEE4bf/GMT86ZWceSEit5vukHBrfRbU1JCyx60j6FdyZaCW/nVHmZf138AD14BT6bMEfToZ+GX8+3MneKBE7+cfC81/VOYUha3Misbb1M4p9+UHGa54z3b4etWVO53FFaCr8Be8c84s/+7h3ulj5yWQn9BoXJyMs3l3tjmVuDFtTYg3bgFLryj9+eC5b1TMP5C20qIdSVbAf6gDV7uebrHO/Ky5EgoNxAddCb801O2NTDx+GR6zX2snJK+/AXl9t881OQEhZSWgtsP409T4fcqe5rRR+LRtSZUTtKgkMbf1jZS19zFP504xW7obIIHLrf3A7idrm1OUHD7CcQDyx+1z7uabdDwFdor1E//0Xbogp2GIR629x+4o1466u2olorJcPw/245XV2Fl+uelY+zr1MrJTSk1ruqd/3c/V5jh2g096aMMWgpgWx+QHLpZNs7+e7iBK1hmg9Ge+IvsEM9wuw0Grpnn2Y7e1G1gg0VBub2vIvVYY46Aa/+S7LdwO6arp5OWx2ODZteuZEcz9D5euqm03XSRv9h+R9/0kbYSVI7S9FEaS9Y0UuDzcNpMp6LZ8hqsedZW5O60CO1O+qhli70KLZtgU0hgrzp9QVtxi8C0U51J4sROAVEy2k6J8ORX4bL77fPZn7J9BX2ltg6CKc/nfxOO6LNiqRsUEjF7bFeREwyK0nQ6p1NzsL3KHz8nWbmV72E47rij4TNPJkcWef32Dmp31E4m3Cttk7DB1HXy1+1fX3M/a1NGvqBtqfQ9NzcAzroAJv8AJhzb/7GLqpMthYATFFLLkC415AYPNyhrUFAHCA0KabyydifzplYl71x2+w/atqX0KWy3I49aNtsKc/qClKDgjNlPveL3BWx6x703oGYG/OlL8MLNtuUQ7JOmcnmcsf/drb2v9Kum2r9UqRVjr5ZCSvooE6Wj4QbnxrhEAk6/GQ772J4/M+0jvV9f+5fMjuXyp1TCfVsF6Xi8yfOdmKbCd98LlvfuWE6nqMr+Zn37FHrKs4eg4AYMX8C2lEzcBja9cU3lKE0f9VHf1s3ahg5OPihlFs6eoLA1GRTiYfu8dYtNVaRWPKEmmz5KvVoHuOQu+M4Ou4DL4R+zwyHdu26D5f0XqrDS5sbTXbGm8vqTFX/qsfc1fZTK44H5/5LsfB4qqRVv35FFA+Gea0GaUUF9FVWnBIWUPgXXntJHvUYhFSdba9pSUDkqK0FBRL4uIh+IyHIReVBEgiIyVUReF5F1IvKwiGTl/6rXP7QjTk5KDQo9ncpbe8+q2bzRvlcx0U4qd8Eddjx9V7MNJH3H9UPvzt2i6uRUFoX9tBTAXum7N4btjVsppR7bvWpOd8/CSJFa8foyaCnsjXuuqaOE+t230gZ8E++npZAmKKRLGwVKkn0b2lJQOWrYg4KIjAe+Csw1xhwOeIHLgR8BPzHGHAQ0A9cOd9kA1ta34/XAwWxMbuybPipxOl+3OtNRuEM/j7na5tVN3I6i2VPnLNg7gt2plvfWUugvvbTbd7pBIaWlMG6OnZKifHxm35ENvdJHhf3vlym3pZDu/oG+iqqTAwgC6VoKaVpo/mJAerdEgmVQ6qQMtaWgclS20kc+oFBEfEARsB04DXCG73A/cHE2Cra2voOPl68mcM8psNNZmcttKbhBwZ2gbaez7GLqXEGpV+PuHD79Se0DCO4htXPSV+G072R2AsUpI4dcU+fbBXn2ln7KptT00aC0FPYlfZTyO7hDV92WgjeQ/qrfHXGUGnTOvx3O+J5N9WlQUDlq2IOCMWYr8GNgMzYYtAJvAS3GGHdCoTog7WWtiCwUkaUisrSxMc2U0ftpbUM7h5U400y7i6i4w0/bt9mhi24Hr7skZupVfGoFM2ZvQSElRbWnlsK0U+GwS/ZScke6lkIuSE0fDUafgtsHkkn6KDWou/c9uK2VPZWlcnLvSQInHWdvcvMXa/pI5axspI8qgYuAqcA4oBg4O9PPG2PuNsbMNcbMHTVq1KCWLRyLs7EpxMRiJzZ1t9nOx0i7nWEzEbPz6VRMBiQ5135qf0CvVsNeOnZ77ZthemhvKibbisxNceWK1Mo3k9FHe3PwOXDuj/feWoPerTs3mLjpoz21rq55BhakacH5C7WloHJWNoakngF8aIxpBBCRPwInARUi4nNaCxOANKuwDK2NO0PEE4ZxQWcitXBbspUw4Zjk3ctFVfbK3p14LrWlsC8jfFKDwp5aCvti3uftDV97u1lspEkNCr5B6FMIFNl/i0y4v0Pqb+AGpj0Fhf5+s0BRcr4ppXJMNvoUNgPHi0iR2KlHTwdWAIuBS519rgaeGO6CrWuw0xrU+Jw1hbvbkv0Jh5yb3DFYYa/s3Vk4U6/y3Zz+5JP3fkB338FMN/gL+797dyQLDHJLYV+4/Q6ps9L6Mkgf9cdfpOkjlbOGvaVgjHldRB4F3gZiwDvA3cDTwEMi8l/OtvuGu2zbW0KU0UGZOCOCwq3JRd/HHAn//Kpdh3fiPNsiaN5orwhTR8sEy+EzT2R2N2+6K9R85R/kIan7ovZQOP8nvW/Qy6Sl0J/DLs6sL0OpESgrdzQbY24Gbu6zeQMwLwvF6VG0ZQlLC76Jv9mZfrm7LTnlc/Eom292F5PvuSEsTV/AtFMzPGB1/9+Rb7z+5Iyog9HRvC9E7LQZqTLpU+hP6hTbSuUYvaM5RbBlDQGJI+56BOE221IQz+59BW4/Qqb3D6Q9YHlyCUuVTCENd/ooHbcMwx2glMoynfsohc9dvtG9oSzcbuc3Kqq249JT7amlkCkR+90aFCx/kZ3jaTA6mvfX/rQUlMphGhRSBMJ9lnrsboNYODn2P5UbFPanpQB2Fs/UhXTymb8Iu8jOCBi5oy0Flac0KDiMMZRFm3on1MJtECY5SiiV20LY3/4Ad50FZdNH/sLM5ngaatpSUHlK+xQcuzojVNPSe2O306cwlC0FleQvGv6RR/3Zn9FHSuUwbSk4drR1M076BIVwm13MvShdS2E/pqNW6fmLBmcyvMFQUAbHft4uW6pUHtGg4GhobuMw6Uhu8PhtKyEeSd9SCA5S+kglBYpHTktBBM77cbZLodSw06Dg6Giy02MbBMHYVdPcaSzS9Sn03GMwgtcoyDXHfdGuV62UyhoNCo5Yi53OwlRNQ3att0ts9gSFNC2FUYfARb+AWecPYykPcFPnZ7sESuU97Wh2JNrtFaqMPdJuSF2QJt001CJw9JXaEamUOqBoUHAUdNgZUGW8M8WFO/V02YTkHPtKKXWA0/SRo6pjLU1SRfXsK+0i7sd+znY0n3HL7nczK6XUAUqDgmNM93q2+KdQXVQFZzhz9V1yV3YLpZRSw0wvgQHiMSZEN1FfeFC2S6KUUlmlQQFg13oCRGkumZHtkiilVFZlFBRE5I8icp6IHJhBpP4DADoqdGI6pVR+y7SS/wVwBbBWRG4VkQOq9ox27ALAV1ab5ZIopVR2ZRQUjDGLjDFXAnOAjcAiEXlVRK4RkZxfjLaruxuA0iKdJlkpld8yTgeJSDXwT8DnsGso/xQbJF4YkpINo66wExSKR8hkbEoplSUZDUkVkceBQ4DfAhcYY7Y7bz0sIkuHqnDDpdtpKZRpUFBK5blM71O4wxizON0bxpi5+3JApz/i4ZRN04B/B37jbJ+CTVFdZoxp3pfvHqhwOAxAWbFOWaGUym+Zpo8OFZGeOaJFpFJEvjSQAxpjVhtjZhtjZgPHACHgceBG4EVjzAzgRef1sIhEbFCoLBkh0zYrpVSWZBoUPm+M6VmBxrmC//wgHP90YL0xZhNwEXC/s/1+4OJB+P6MRMJhIsZLRdEIWBtYKaWyKNOg4BVJLpwrIl4gMAjHvxx40Hk+OqWvYgcwOt0HRGShiCwVkaWNjY2DUASIRsNE8RH0H5i3YSilVKYyrQWfw3Yqny4ip2Mr8uf258AiEgAuBP7Q9z1jjAFMus8ZY+42xsw1xswdNSrNOgcDEItEiIsPGQkLxiulVBZl2tH8LeALwD87r18A7t3PY58DvG2McZfaqheRscaY7SIyFmjYz+/PWCwaJiE6N6BSSmVUExpjEsCdzt9g+RTJ1BHAk8DVwK3O4xODeKw9iscixHP/HjyllNpvmd6nMAP4IXAo0DNExxgzbSAHFZFi4Exs68N1K/CIiFwLbAIuG8h3D0Q8FsV4tKWglFKZ1oT/B9wM/ARYAFzDfsywaozpBKr7bGvCjkYadiYWwfi1paCUUplW7IXGmBcBMcZsMsZ8Dzhv6Io1fIwxmHgUvBoUlFIq05ZC2Jk2e62IfBnYCpQMXbGGT1c0jsfENCgopRSZtxSuB4qAr2LvQr4K2xmc81pCUQLE8HgH47YLpZTKbXttKTg3qn3SGPNNoAPbn3DAaA5F8BHH49O7mZVSaq8tBWNMHDh5GMqSFa2hKD6J4/FpS0EppTLtU3hHRJ7E3n3c6W40xvxxSEo1jJpDUcYRw+vXoKCUUpkGhSDQBJyWss0AOR8UWruiTCKGT4OCUkplfEfzAdWPkKozHMNHHK+mj5RSKuM7mv+PNBPUGWM+O+glGmahSJwAMbx+7WhWSqlM00dPpTwPApcA2wa/OMMvFI3hlzgevU9BKaUyTh89lvpaRB4EXhmSEg2zrkgcv8T15jWllGLg8xfNAGoHsyDZ0hWJE0CDglJKQeZ9Cu307lPYgV1jIeeFonF8xMGjQUEppTJNH5UOdUGyxaaPdO4jpZSCDNNHInKJiJSnvK4QkYuHrljDJxSJ4dMJ8ZRSCsi8T+FmY0yr+8IY04JdXyHndUU0faSUUq5Mg0K6/Q6Ipcq6wxE8JLSloJRSZB4UlorIbSIy3fm7DXhrKAs2XCKRiH2iQUEppTIOCl8BIsDDwENAN3DdUBVqOEWjTlDQ9JFSSmU8+qgTuHGwDioiFcC9wOHYoa6fBVZjg84UYCNwmTGmebCO2Z9oNAxetKWglFJkPvroBacid19Xishf9uO4PwWeM8bMBI4CVmKDzovGmBnAiwxiEOpPImFIRDV9pJRSrkzTRzXOiCMAnCv4Ad3R7AxtPQW4z/muiPPdFwH3O7vdDwz5kNcu98Y10PSRUkqReVBIiMgk94WITCHNrKkZmgo0Av8nIu+IyL0iUgyMNsZsd/bZAYwe4PdnLBSJ45OYfaEtBaWUynhY6XeAV0RkCSDAfGDhfhxzDvAVY8zrIvJT+qSKjDFGRNIGHRFZ6B570qRJ6XbJWJczbTYAXl1PQSmlMmopGGOeA+ZiO4MfBL4BdA3wmHVAnTHmdef1o9ggUS8iYwGcx4Z+ynK3MWauMWbuqFGjBlgEK/DOvbxYcIN94TkgbrtQSqn9kumEeJ8DrgcmAMuA44F/0Ht5zowYY3aIyBYROcQYsxo4HVjh/F0N3Oo8PrGv372vylY+lHyh6SOllMo4fXQ9cCzwmjFmgYjMBH6wH8f9CvB7EQkAG4BrsK2WR0TkWmATcNl+fH9Gor6S5AtNHymlVMZBodsY0y0iiEiBMWaViBwy0IMaY5Zh01F9nT7Q7xyIsDclKGj6SCmlMg4Kdc59Cn8CXhCRZuzVfE4Le4uTL7SloJRSGd/RfInz9HsishgoB54bslINkwgp/Qjap6CUUvs+06kxZslQFCQbTKw7+ULTR0opNeA1mg8InljKqFpNHymlVL4HhZSWgqaPlFIqz4NCXNNHSimVKq+Dgjc1KEhe/1MopRSQ90EhnHzhL8xeQZRSaoTI66DgS3SzyHMifOVtKB2T7eIopVTW5XlQCNPtKYLq6dkuilJKjQh5HRT8iTAxT0G2i6GUUiNGfgcFEyauQUEppXrkb1BIJAiYCDGvdjArpZQrf4OCc+NawqstBaWUcmlQ8AazXBCllBo58jcoRO28R3GfBgWllHLlfVDAp30KSinlyt+g4MyQarSloJRSPfI3KESdeY90egullOqRx0EhZB81KCilVI+szBctIhuBdiAOxIwxc0WkCngYmAJsBC4zxjQPWSGc0UeiQUEppXpks6WwwBgz2xgz13l9I/CiMWYG8KLzesgkIrZPQYOCUkoljaT00UXA/c7z+4GLh/JgsYhNH3kCRUN5GKWUyinZCgoGeF5E3hKRhc620caY7c7zHcDodB8UkYUislREljY2Ng64APHuTgA8AW0pKKWUK1trUJ5sjNkqIrXACyKyKvVNY4wREZPug8aYu4G7AebOnZt2n0y4LQWvthSUUqpHVloKxpitzmMD8DgwD6gXkbEAzmPDUJYhHnZaCgXFQ3kYpZTKKcMeFESkWERK3efAWcBy4Engame3q4EnhrIcpruDiPESKNCb15RSypWN9NFo4HERcY//gDHmORF5E3hERK4FNgGXDWUhEpFOQgQp8I2kvnallMquYQ8KxpgNwFFptjcBpw9bOSIhQhRoUFBKqRT5WyNGOukyBQQ0KCilVI+8rREl2um0FLzZLopSSo0YeRwUQtqnoJRSfeRtjeiJhgiZAoL+vP0nUEqp3eRtjeiJ2Y7mgFfTR0op5crboOCNdREyQQq0paCUUj3ytkb0xt2WQt7+Eyil1G7ytkb0xbvoQlsKSimVKj9rxHgMXyJCpynAry0FpZTqkZ81YtROhheiAJ9HslwYpZQaOfIzKDjTZkc8hThzMCmllCJfg0I0GRSUUkol5WdQiHTYB49Om62UUqnyNCjYlkLMqy0FpZRKlZ9Bwelo1vSRUkr1lp9BIWKDQtyn6zMrpVSqPA0KNn0U1/SRUkr1kp9BwUkfxbSloJRSveRnUHBaCgkNCkop1cuwr9E8Isw4i5+/0UrCp0NSlVIqVdZaCiLiFZF3ROQp5/VUEXldRNaJyMMiEhiyg9fOZFHgNHy+/IyJSinVn2ymj64HVqa8/hHwE2PMQUAzcO1QHjwaT+hSnEop1UdWakURmQCcB9zrvBbgNOBRZ5f7gYuHsgyRWEJnSFVKqT6yVSveDvwrkHBeVwMtxpiY87oOGJ/ugyKyUESWisjSxsbGARcgGtegoJRSfQ17rSgi5wMNxpi3BvJ5Y8zdxpi5xpi5o0aNGnA5onFDQNNHSinVSzZ6Wk8CLhSRc4EgUAb8FKgQEZ/TWpgAbB3KQoQ1faSUUrsZ9lrRGPNtY8wEY8wU4HLgJWPMlcBi4FJnt6uBJ4ayHNF4goBX11JQSqlUI+lS+VvAv4jIOmwfw31DebBoPKHpI6WU6iOrA/WNMS8DLzvPNwDzhuvYOvpIKaV2l5e1YiJhiCWMBgWllOojL2vFaMKOhNX0kVJK9ZaXtWIk5gQFbSkopVQveVkrRuMG0JaCUkr1lZe1YjRuWwrap6CUUr3lZa3opo/8ep+CUkr1kp9BIa4dzUoplU5e1ora0ayUUunlZa2ofQpKKZVeXtaKUU0fKaVUWnlZK4Zj2lJQSql08rJWTN6noKOPlFIqVX4GhZ6OZm+WS6KUUiNLXgYFd0iqX1sKSinVS14GBR19pJRS6eVlraj3KSilVHp5WSvqHc1KKZVeXtaKUR2SqpRSaeVlrahTZyulVHp5WStOri7i3CPGaJ+CUkr14RvuA4pIEPgrUOAc/1FjzM0iMhV4CKgG3gI+bYyJDEUZzjpsDGcdNmYovloppXJaNi6Vw8BpxpijgNnA2SJyPPAj4CfGmIOAZuDaLJRNKaXy2rAHBWN1OC/9zp8BTgMedbbfD1w83GVTSql8l5Wkuoh4RWQZ0AC8AKwHWowxMWeXOmB8P59dKCJLRWRpY2Pj8BRYKaXyRFaCgjEmboyZDUwA5gEz9+Gzdxtj5hpj5o4aNWrIyqiUUvkoq8NvjDEtwGLgBKBCRNyO7wnA1qwVTCml8tSwBwURGSUiFc7zQuBMYCU2OFzq7HY18MRwl00ppfLdsA9JBcYC94uIFxuUHjHGPCUiK4CHROS/gHeA+7JQNqWUymvDHhSMMe8BR6fZvgHbv6CUUipLxBiT7TIMmIg0ApsG8NEaYOcgFydb9FxGJj2XkUnPxZpsjEk7Uieng8JAichSY8zcbJdjMOi5jEx6LiOTnsve6eQ/SimlemhQUEop1SNfg8Ld2S7AINJzGZn0XEYmPZe9yMs+BaWUUunla0tBKaVUGhoUlFJK9ci7oCAiZ4vIahFZJyI3Zrs8+0pENorI+yKyTESWOtuqROQFEVnrPFZmu5zpiMivRKRBRJanbEtbdrHucH6n90RkTvZKvrt+zuV7IrLV+W2Wici5Ke992zmX1SLy0eyUenciMlFEFovIChH5QESud7bn3O+yh3PJxd8lKCJviMi7zrnc4myfKiKvO2V+WEQCzvYC5/U65/0pAz64MSZv/gAvdpruaUAAeBc4NNvl2sdz2AjU9Nn238CNzvMbgR9lu5z9lP0UYA6wfG9lB84FngUEOB54Pdvlz+Bcvgd8M82+hzr/rRUAU53/Br3ZPgenbGOBOc7zUmCNU96c+132cC65+LsIUOI89wOvO//ejwCXO9vvAv7Zef4l4C7n+eXAwwM9dr61FOYB64wxG4xd6vMh4KIsl2kwXIRdmAhG8AJFxpi/Arv6bO6v7BcBvzHWa9hZdMcOT0n3rp9z6c9FwEPGmLAx5kNgHSNkShdjzHZjzNvO83bs5JTjycHfZQ/n0p+R/LsYs2+LkaX+Xo8Cp4uIDOTY+RYUxgNbUl73u5jPCGaA50XkLRFZ6GwbbYzZ7jzfAYzOTtEGpL+y5+pv9WUnrfKrlDReTpyLk3I4GntVmtO/S59zgRz8XfZxMbKec3Heb8Wud7/P8i0oHAhONsbMAc4BrhORU1LfNLb9mJPjjHO57I47genYtce3A/8vu8XJnIiUAI8BXzPGtKW+l2u/S5pzycnfxezHYmT7I9+CwlZgYsrrnFvMxxiz1XlsAB7H/sdS7zbhnceG7JVwn/VX9pz7rYwx9c7/yAngHpKpiBF9LiLix1aivzfG/NHZnJO/S7pzydXfxWUyW4ys51yc98uBpoEcL9+CwpvADKcHP4DtkHkyy2XKmIgUi0ip+xw4C1iOPYernd1ybYGi/sr+JPAZZ7TL8UBrSjpjROqTW78E+9uAPZfLnREiU4EZwBvDXb50nLzzfcBKY8xtKW/l3O/S37nk6O+yr4uRpf5elwIvOS28fZftXvbh/sOOnliDzc99J9vl2ceyT8OOlngX+MAtPzZ3+CKwFlgEVGW7rP2U/0Fs8z2KzYde21/ZsaMv/tf5nd4H5ma7/Bmcy2+dsr7n/E86NmX/7zjnsho4J9vlTynXydjU0HvAMufv3Fz8XfZwLrn4uxyJXWzsPWwQ+3dn+zRs4FoH/AEocLYHndfrnPenDfTYOs2FUkqpHvmWPlJKKbUHGhSUUkr10KCglFKqhwYFpZRSPTQoKKWU6qFBQaksEZFTReSpbJdDqVQaFJRSSvXQoKDUXojIVc7c9stE5JfORGUdIvITZ677F0VklLPvbBF5zZl87fGUdQgOEpFFzvz4b4vIdOfrS0TkURFZJSK/H+jMlkoNFg0KSu2BiMwCPgmcZOzkZHHgSqAYWGqMOQxYAtzsfOQ3wLeMMUdi76J1t/8e+F9jzFHAidi7ocHO5Pk17Nz+04CThvyklNoD3953USqvnQ4cA7zpXMQXYieHSwAPO/v8DvijiJQDFcaYJc72+4E/OPNVjTfGPA5gjOkGcL7vDWNMnfN6GTAFeGXoT0up9DQoKLVnAtxvjPl2r40iN/XZb6DzxYRTnsfR/ydVlmn6SKk9exG4VERqoWft4snY/3fc2SqvAF4xxrQCzSIy39n+aWCJsauA1YnIxc53FIhI0bCehVIZ0qsSpfbAGLNCRL6LXe3Og50V9TqgE5jnvNeA7XcAO33xXU6lvwG4xtn+aeCXIvIfznd8YhhPQ6mM6SypSg2AiHQYY0qyXQ6lBpumj5RSSvXQloJSSqke2lJQSinVQ4OCUkqpHhoUlFJK9dCgoJRSqocGBaWUUj3+Py+fVabov7WjAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_val = 'adam'\n",
        "EPOCH_LIST, TRAIN_ACC_LIST, TEST_ACC_LIST = main(lr_val, start_epoch)"
      ],
      "metadata": {
        "id": "bXihymYJg8Il",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a10897a9c3af41e0904881011d85b276",
            "48c42dd92b6a4597a677efdf0be2e84a",
            "603abb200d7d4befa04accdc6035253b",
            "8534bd3ac97a4258acadc6a6f4f7103c",
            "4b38a28479234d368efba6ac7e11146a",
            "fcfe744ad59c40798abc6ecba4b13224",
            "4a294002808c4652bd1a6422535ef2f1",
            "0f8be2375184468fb8fdd1cf2eaef168",
            "4ed0b0d035d74f9fb728648a38b820b9",
            "c6bf8fceab97467ca505a76fdc4cf4ff",
            "9508406ae82d4acfb7dea8be8fb0e5b2"
          ]
        },
        "outputId": "4e8fa495-d889-4fcc-9f8f-d3df2ab8e71a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a10897a9c3af41e0904881011d85b276"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "e: 0 loss: 2.30271 train_err: 89.748 train_acc: 10.252 test_top1: 89.390 test_loss 2.30278 test_acc: 10.610\n",
            "\n",
            "e: 1 loss: 2.22386 train_err: 77.128 train_acc: 22.872 test_top1: 82.850 test_loss 2.40879 test_acc: 17.150\n",
            "\n",
            "e: 2 loss: 1.83520 train_err: 70.552 train_acc: 29.448 test_top1: 75.030 test_loss 2.05760 test_acc: 24.970\n",
            "\n",
            "e: 3 loss: 1.82933 train_err: 70.488 train_acc: 29.512 test_top1: 82.670 test_loss 4.69762 test_acc: 17.330\n",
            "\n",
            "e: 4 loss: 1.82238 train_err: 70.056 train_acc: 29.944 test_top1: 74.350 test_loss 1.90460 test_acc: 25.650\n",
            "\n",
            "e: 5 loss: 1.81695 train_err: 70.000 train_acc: 30.000 test_top1: 77.460 test_loss 2.87824 test_acc: 22.540\n",
            "\n",
            "e: 6 loss: 1.81847 train_err: 70.462 train_acc: 29.538 test_top1: 76.990 test_loss 4.71491 test_acc: 23.010\n",
            "\n",
            "e: 7 loss: 1.81907 train_err: 69.838 train_acc: 30.162 test_top1: 78.690 test_loss 2.70977 test_acc: 21.310\n",
            "\n",
            "e: 8 loss: 1.82058 train_err: 70.182 train_acc: 29.818 test_top1: 75.800 test_loss 2.69742 test_acc: 24.200\n",
            "\n",
            "e: 9 loss: 1.82043 train_err: 69.954 train_acc: 30.046 test_top1: 87.370 test_loss 2.91827 test_acc: 12.630\n",
            "\n",
            "e: 10 loss: 1.82609 train_err: 70.088 train_acc: 29.912 test_top1: 76.540 test_loss 2.53348 test_acc: 23.460\n",
            "\n",
            "e: 11 loss: 1.82279 train_err: 69.652 train_acc: 30.348 test_top1: 87.160 test_loss 3.71608 test_acc: 12.840\n",
            "\n",
            "e: 12 loss: 1.82120 train_err: 70.106 train_acc: 29.894 test_top1: 83.000 test_loss 11.59444 test_acc: 17.000\n",
            "\n",
            "e: 13 loss: 1.82193 train_err: 70.330 train_acc: 29.670 test_top1: 79.660 test_loss 2.81909 test_acc: 20.340\n",
            "\n",
            "e: 14 loss: 1.82850 train_err: 70.528 train_acc: 29.472 test_top1: 83.530 test_loss 3.79838 test_acc: 16.470\n",
            "\n",
            "e: 15 loss: 1.82977 train_err: 70.516 train_acc: 29.484 test_top1: 77.800 test_loss 2.05617 test_acc: 22.200\n",
            "\n",
            "e: 16 loss: 1.82632 train_err: 70.580 train_acc: 29.420 test_top1: 89.210 test_loss 4.86310 test_acc: 10.790\n",
            "\n",
            "e: 17 loss: 1.82457 train_err: 70.318 train_acc: 29.682 test_top1: 77.650 test_loss 5.35846 test_acc: 22.350\n",
            "\n",
            "e: 18 loss: 1.83350 train_err: 70.354 train_acc: 29.646 test_top1: 79.090 test_loss 2.47452 test_acc: 20.910\n",
            "\n",
            "e: 19 loss: 1.82333 train_err: 70.168 train_acc: 29.832 test_top1: 90.000 test_loss 78.07234 test_acc: 10.000\n",
            "\n",
            "e: 20 loss: 1.84304 train_err: 71.274 train_acc: 28.726 test_top1: 90.050 test_loss 36.02701 test_acc: 9.950\n",
            "\n",
            "e: 21 loss: 1.83635 train_err: 70.556 train_acc: 29.444 test_top1: 79.630 test_loss 2.18436 test_acc: 20.370\n",
            "\n",
            "e: 22 loss: 1.83099 train_err: 70.342 train_acc: 29.658 test_top1: 87.280 test_loss 5.15100 test_acc: 12.720\n",
            "\n",
            "e: 23 loss: 1.82609 train_err: 70.090 train_acc: 29.910 test_top1: 90.300 test_loss 7.85762 test_acc: 9.700\n",
            "\n",
            "e: 24 loss: 1.83859 train_err: 70.656 train_acc: 29.344 test_top1: 79.570 test_loss 4.14526 test_acc: 20.430\n",
            "\n",
            "e: 25 loss: 1.83872 train_err: 70.750 train_acc: 29.250 test_top1: 88.310 test_loss 3.81213 test_acc: 11.690\n",
            "\n",
            "e: 26 loss: 1.84028 train_err: 71.106 train_acc: 28.894 test_top1: 80.940 test_loss 2.54334 test_acc: 19.060\n",
            "\n",
            "e: 27 loss: 1.83886 train_err: 70.664 train_acc: 29.336 test_top1: 81.190 test_loss 3.69432 test_acc: 18.810\n",
            "\n",
            "e: 28 loss: 1.83586 train_err: 70.300 train_acc: 29.700 test_top1: 89.850 test_loss 10.89879 test_acc: 10.150\n",
            "\n",
            "e: 29 loss: 1.85088 train_err: 71.008 train_acc: 28.992 test_top1: 78.930 test_loss 2.22086 test_acc: 21.070\n",
            "\n",
            "e: 30 loss: 1.83973 train_err: 70.874 train_acc: 29.126 test_top1: 82.990 test_loss 5.19539 test_acc: 17.010\n",
            "\n",
            "e: 31 loss: 1.84884 train_err: 70.756 train_acc: 29.244 test_top1: 78.190 test_loss 2.56276 test_acc: 21.810\n",
            "\n",
            "e: 32 loss: 1.84719 train_err: 71.168 train_acc: 28.832 test_top1: 74.930 test_loss 2.80044 test_acc: 25.070\n",
            "\n",
            "e: 33 loss: 1.85088 train_err: 71.188 train_acc: 28.812 test_top1: 88.080 test_loss 15.69910 test_acc: 11.920\n",
            "\n",
            "e: 34 loss: 1.83892 train_err: 70.854 train_acc: 29.146 test_top1: 81.870 test_loss 4.05021 test_acc: 18.130\n",
            "\n",
            "e: 35 loss: 1.84777 train_err: 70.722 train_acc: 29.278 test_top1: 88.220 test_loss 6.51382 test_acc: 11.780\n",
            "\n",
            "e: 36 loss: 1.83661 train_err: 70.446 train_acc: 29.554 test_top1: 78.160 test_loss 2.75453 test_acc: 21.840\n",
            "\n",
            "e: 37 loss: 1.84074 train_err: 70.524 train_acc: 29.476 test_top1: 84.780 test_loss 3.65217 test_acc: 15.220\n",
            "\n",
            "e: 38 loss: 1.84898 train_err: 70.772 train_acc: 29.228 test_top1: 80.360 test_loss 2.58720 test_acc: 19.640\n",
            "\n",
            "e: 39 loss: 1.84401 train_err: 70.606 train_acc: 29.394 test_top1: 85.750 test_loss 2.68789 test_acc: 14.250\n",
            "\n",
            "e: 40 loss: 1.84823 train_err: 70.794 train_acc: 29.206 test_top1: 90.050 test_loss 18.19321 test_acc: 9.950\n",
            "\n",
            "e: 41 loss: 1.84351 train_err: 70.366 train_acc: 29.634 test_top1: 80.570 test_loss 2.36543 test_acc: 19.430\n",
            "\n",
            "e: 42 loss: 1.84217 train_err: 70.568 train_acc: 29.432 test_top1: 87.880 test_loss 2.77639 test_acc: 12.120\n",
            "\n",
            "e: 43 loss: 1.84041 train_err: 70.386 train_acc: 29.614 test_top1: 88.770 test_loss 6.79079 test_acc: 11.230\n",
            "\n",
            "e: 44 loss: 1.83589 train_err: 70.188 train_acc: 29.812 test_top1: 78.070 test_loss 2.32278 test_acc: 21.930\n",
            "\n",
            "e: 45 loss: 1.84679 train_err: 70.634 train_acc: 29.366 test_top1: 87.560 test_loss 7.37325 test_acc: 12.440\n",
            "\n",
            "e: 46 loss: 1.84812 train_err: 70.812 train_acc: 29.188 test_top1: 83.840 test_loss 3.05965 test_acc: 16.160\n",
            "\n",
            "e: 47 loss: 1.83969 train_err: 70.376 train_acc: 29.624 test_top1: 75.710 test_loss 2.14580 test_acc: 24.290\n",
            "\n",
            "e: 48 loss: 1.83345 train_err: 69.796 train_acc: 30.204 test_top1: 79.180 test_loss 2.31794 test_acc: 20.820\n",
            "\n",
            "e: 49 loss: 1.82641 train_err: 69.948 train_acc: 30.052 test_top1: 74.890 test_loss 2.12974 test_acc: 25.110\n",
            "\n",
            "e: 50 loss: 1.82843 train_err: 69.642 train_acc: 30.358 test_top1: 81.480 test_loss 4.00002 test_acc: 18.520\n",
            "\n",
            "e: 51 loss: 1.82175 train_err: 69.692 train_acc: 30.308 test_top1: 81.690 test_loss 3.56155 test_acc: 18.310\n",
            "\n",
            "e: 52 loss: 1.82503 train_err: 69.932 train_acc: 30.068 test_top1: 89.730 test_loss 3.23965 test_acc: 10.270\n",
            "\n",
            "e: 53 loss: 1.82596 train_err: 69.556 train_acc: 30.444 test_top1: 79.360 test_loss 2.57686 test_acc: 20.640\n",
            "\n",
            "e: 54 loss: 1.81892 train_err: 69.398 train_acc: 30.602 test_top1: 85.880 test_loss 2.54858 test_acc: 14.120\n",
            "\n",
            "e: 55 loss: 1.82282 train_err: 69.536 train_acc: 30.464 test_top1: 86.610 test_loss 7.51710 test_acc: 13.390\n",
            "\n",
            "e: 56 loss: 1.82408 train_err: 69.924 train_acc: 30.076 test_top1: 83.760 test_loss 3.16681 test_acc: 16.240\n",
            "\n",
            "e: 57 loss: 1.82837 train_err: 69.736 train_acc: 30.264 test_top1: 90.760 test_loss 8.94860 test_acc: 9.240\n",
            "\n",
            "e: 58 loss: 1.83276 train_err: 69.896 train_acc: 30.104 test_top1: 83.250 test_loss 5.10340 test_acc: 16.750\n",
            "\n",
            "e: 59 loss: 1.82696 train_err: 69.680 train_acc: 30.320 test_top1: 82.080 test_loss 9.88724 test_acc: 17.920\n",
            "\n",
            "e: 60 loss: 1.83013 train_err: 69.990 train_acc: 30.010 test_top1: 84.310 test_loss 3.77206 test_acc: 15.690\n",
            "\n",
            "e: 61 loss: 1.82857 train_err: 70.004 train_acc: 29.996 test_top1: 84.350 test_loss 17.30504 test_acc: 15.650\n",
            "\n",
            "e: 62 loss: 1.82793 train_err: 69.668 train_acc: 30.332 test_top1: 90.000 test_loss 24.15601 test_acc: 10.000\n",
            "\n",
            "e: 63 loss: 1.81642 train_err: 69.432 train_acc: 30.568 test_top1: 81.210 test_loss 4.42917 test_acc: 18.790\n",
            "\n",
            "e: 64 loss: 1.82564 train_err: 69.784 train_acc: 30.216 test_top1: 93.140 test_loss 6.25718 test_acc: 6.860\n",
            "\n",
            "e: 65 loss: 1.82355 train_err: 69.990 train_acc: 30.010 test_top1: 88.930 test_loss 3.93206 test_acc: 11.070\n",
            "\n",
            "e: 66 loss: 1.82568 train_err: 69.946 train_acc: 30.054 test_top1: 79.260 test_loss 4.86588 test_acc: 20.740\n",
            "\n",
            "e: 67 loss: 1.82454 train_err: 69.736 train_acc: 30.264 test_top1: 77.170 test_loss 2.39160 test_acc: 22.830\n",
            "\n",
            "e: 68 loss: 1.82315 train_err: 69.346 train_acc: 30.654 test_top1: 84.370 test_loss 7.62234 test_acc: 15.630\n",
            "\n",
            "e: 69 loss: 1.82701 train_err: 69.446 train_acc: 30.554 test_top1: 87.560 test_loss 39.61422 test_acc: 12.440\n",
            "\n",
            "e: 70 loss: 1.82643 train_err: 69.762 train_acc: 30.238 test_top1: 86.100 test_loss 3.39564 test_acc: 13.900\n",
            "\n",
            "e: 71 loss: 1.82481 train_err: 69.842 train_acc: 30.158 test_top1: 90.000 test_loss 17.51021 test_acc: 10.000\n",
            "\n",
            "e: 72 loss: 1.84104 train_err: 70.090 train_acc: 29.910 test_top1: 81.590 test_loss 3.07172 test_acc: 18.410\n",
            "\n",
            "e: 73 loss: 1.82665 train_err: 69.686 train_acc: 30.314 test_top1: 81.020 test_loss 3.80853 test_acc: 18.980\n",
            "\n",
            "e: 74 loss: 1.83115 train_err: 69.990 train_acc: 30.010 test_top1: 83.210 test_loss 3.87329 test_acc: 16.790\n",
            "\n",
            "e: 75 loss: 1.82248 train_err: 69.546 train_acc: 30.454 test_top1: 82.110 test_loss 7.22429 test_acc: 17.890\n",
            "\n",
            "e: 76 loss: 1.82762 train_err: 69.896 train_acc: 30.104 test_top1: 79.480 test_loss 2.13568 test_acc: 20.520\n",
            "\n",
            "e: 77 loss: 1.82493 train_err: 69.712 train_acc: 30.288 test_top1: 76.020 test_loss 2.00542 test_acc: 23.980\n",
            "\n",
            "e: 78 loss: 1.83335 train_err: 69.924 train_acc: 30.076 test_top1: 89.480 test_loss 6.08924 test_acc: 10.520\n",
            "\n",
            "e: 79 loss: 1.82804 train_err: 69.922 train_acc: 30.078 test_top1: 84.870 test_loss 9.53153 test_acc: 15.130\n",
            "\n",
            "e: 80 loss: 1.83234 train_err: 70.138 train_acc: 29.862 test_top1: 77.300 test_loss 2.07324 test_acc: 22.700\n",
            "\n",
            "e: 81 loss: 1.82834 train_err: 69.726 train_acc: 30.274 test_top1: 69.360 test_loss 1.88662 test_acc: 30.640\n",
            "\n",
            "e: 82 loss: 1.81826 train_err: 69.418 train_acc: 30.582 test_top1: 80.250 test_loss 3.41865 test_acc: 19.750\n",
            "\n",
            "e: 83 loss: 1.82260 train_err: 69.742 train_acc: 30.258 test_top1: 83.610 test_loss 3.20600 test_acc: 16.390\n",
            "\n",
            "e: 84 loss: 1.83300 train_err: 70.074 train_acc: 29.926 test_top1: 89.500 test_loss 4.10157 test_acc: 10.500\n",
            "\n",
            "e: 85 loss: 1.82635 train_err: 69.844 train_acc: 30.156 test_top1: 90.000 test_loss 96.46734 test_acc: 10.000\n",
            "\n",
            "e: 86 loss: 1.83220 train_err: 69.958 train_acc: 30.042 test_top1: 86.850 test_loss 4.28564 test_acc: 13.150\n",
            "\n",
            "e: 87 loss: 1.82555 train_err: 69.898 train_acc: 30.102 test_top1: 83.840 test_loss 5.36605 test_acc: 16.160\n",
            "\n",
            "e: 88 loss: 1.82559 train_err: 69.834 train_acc: 30.166 test_top1: 76.510 test_loss 2.19644 test_acc: 23.490\n",
            "\n",
            "e: 89 loss: 1.82347 train_err: 69.652 train_acc: 30.348 test_top1: 83.090 test_loss 2.33968 test_acc: 16.910\n",
            "\n",
            "e: 90 loss: 1.82077 train_err: 69.772 train_acc: 30.228 test_top1: 87.700 test_loss 3.34196 test_acc: 12.300\n",
            "\n",
            "e: 91 loss: 1.83959 train_err: 70.620 train_acc: 29.380 test_top1: 89.880 test_loss 9.12157 test_acc: 10.120\n",
            "\n",
            "e: 92 loss: 1.82367 train_err: 69.482 train_acc: 30.518 test_top1: 80.080 test_loss 4.11019 test_acc: 19.920\n",
            "\n",
            "e: 93 loss: 1.82280 train_err: 69.520 train_acc: 30.480 test_top1: 88.680 test_loss 27.17577 test_acc: 11.320\n",
            "\n",
            "e: 94 loss: 1.82900 train_err: 69.720 train_acc: 30.280 test_top1: 89.670 test_loss 15.73020 test_acc: 10.330\n",
            "\n",
            "e: 95 loss: 1.83820 train_err: 70.382 train_acc: 29.618 test_top1: 84.980 test_loss 4.37742 test_acc: 15.020\n",
            "\n",
            "e: 96 loss: 1.82807 train_err: 69.836 train_acc: 30.164 test_top1: 87.560 test_loss 3.45396 test_acc: 12.440\n",
            "\n",
            "e: 97 loss: 1.82414 train_err: 69.750 train_acc: 30.250 test_top1: 89.880 test_loss 6.05400 test_acc: 10.120\n",
            "\n",
            "e: 98 loss: 1.82924 train_err: 70.160 train_acc: 29.840 test_top1: 86.980 test_loss 2.40215 test_acc: 13.020\n",
            "\n",
            "e: 99 loss: 1.82466 train_err: 69.768 train_acc: 30.232 test_top1: 80.680 test_loss 3.26227 test_acc: 19.320\n",
            "\n",
            "e: 100 loss: 1.82110 train_err: 69.872 train_acc: 30.128 test_top1: 79.380 test_loss 2.18708 test_acc: 20.620\n",
            "\n",
            "e: 101 loss: 1.82783 train_err: 69.712 train_acc: 30.288 test_top1: 90.000 test_loss 18.46018 test_acc: 10.000\n",
            "\n",
            "e: 102 loss: 1.82896 train_err: 69.884 train_acc: 30.116 test_top1: 72.540 test_loss 2.23309 test_acc: 27.460\n",
            "\n",
            "e: 103 loss: 1.82688 train_err: 69.570 train_acc: 30.430 test_top1: 82.390 test_loss 2.75864 test_acc: 17.610\n",
            "\n",
            "e: 104 loss: 1.82593 train_err: 69.544 train_acc: 30.456 test_top1: 89.990 test_loss 13.87313 test_acc: 10.010\n",
            "\n",
            "e: 105 loss: 1.82128 train_err: 69.620 train_acc: 30.380 test_top1: 90.000 test_loss 12.01397 test_acc: 10.000\n",
            "\n",
            "e: 106 loss: 1.83289 train_err: 70.124 train_acc: 29.876 test_top1: 78.260 test_loss 2.88568 test_acc: 21.740\n",
            "\n",
            "e: 107 loss: 1.83761 train_err: 70.414 train_acc: 29.586 test_top1: 82.930 test_loss 2.47403 test_acc: 17.070\n",
            "\n",
            "e: 108 loss: 1.82668 train_err: 69.972 train_acc: 30.028 test_top1: 84.200 test_loss 2.67462 test_acc: 15.800\n",
            "\n",
            "e: 109 loss: 1.83012 train_err: 69.816 train_acc: 30.184 test_top1: 83.380 test_loss 3.10364 test_acc: 16.620\n",
            "\n",
            "e: 110 loss: 1.83685 train_err: 70.108 train_acc: 29.892 test_top1: 76.790 test_loss 2.42670 test_acc: 23.210\n",
            "\n",
            "e: 111 loss: 1.81806 train_err: 69.530 train_acc: 30.470 test_top1: 89.980 test_loss 8.63808 test_acc: 10.020\n",
            "\n",
            "e: 112 loss: 1.82231 train_err: 69.470 train_acc: 30.530 test_top1: 87.650 test_loss 2.65383 test_acc: 12.350\n",
            "\n",
            "e: 113 loss: 1.82779 train_err: 69.702 train_acc: 30.298 test_top1: 90.000 test_loss 114.65998 test_acc: 10.000\n",
            "\n",
            "e: 114 loss: 1.83337 train_err: 70.124 train_acc: 29.876 test_top1: 81.940 test_loss 3.81198 test_acc: 18.060\n",
            "\n",
            "e: 115 loss: 1.81982 train_err: 69.266 train_acc: 30.734 test_top1: 90.000 test_loss 13.40092 test_acc: 10.000\n",
            "\n",
            "e: 116 loss: 1.81919 train_err: 69.566 train_acc: 30.434 test_top1: 84.480 test_loss 6.70613 test_acc: 15.520\n",
            "\n",
            "e: 117 loss: 1.83298 train_err: 69.930 train_acc: 30.070 test_top1: 75.370 test_loss 1.98124 test_acc: 24.630\n",
            "\n",
            "e: 118 loss: 1.82288 train_err: 69.702 train_acc: 30.298 test_top1: 82.150 test_loss 3.69592 test_acc: 17.850\n",
            "\n",
            "e: 119 loss: 1.83250 train_err: 69.906 train_acc: 30.094 test_top1: 89.870 test_loss 15.68115 test_acc: 10.130\n",
            "\n",
            "e: 120 loss: 1.82687 train_err: 69.678 train_acc: 30.322 test_top1: 82.550 test_loss 2.34646 test_acc: 17.450\n",
            "\n",
            "e: 121 loss: 1.82219 train_err: 69.392 train_acc: 30.608 test_top1: 76.500 test_loss 2.65188 test_acc: 23.500\n",
            "\n",
            "e: 122 loss: 1.82358 train_err: 69.850 train_acc: 30.150 test_top1: 89.480 test_loss 3.70295 test_acc: 10.520\n",
            "\n",
            "e: 123 loss: 1.82480 train_err: 69.660 train_acc: 30.340 test_top1: 79.900 test_loss 2.10004 test_acc: 20.100\n",
            "\n",
            "e: 124 loss: 1.83085 train_err: 70.048 train_acc: 29.952 test_top1: 88.110 test_loss 3.55827 test_acc: 11.890\n",
            "\n",
            "e: 125 loss: 1.83569 train_err: 70.168 train_acc: 29.832 test_top1: 84.670 test_loss 6.31227 test_acc: 15.330\n",
            "\n",
            "e: 126 loss: 1.82187 train_err: 69.652 train_acc: 30.348 test_top1: 79.850 test_loss 2.20841 test_acc: 20.150\n",
            "\n",
            "e: 127 loss: 1.82313 train_err: 69.506 train_acc: 30.494 test_top1: 82.780 test_loss 3.40088 test_acc: 17.220\n",
            "\n",
            "e: 128 loss: 1.83592 train_err: 70.216 train_acc: 29.784 test_top1: 83.150 test_loss 5.39380 test_acc: 16.850\n",
            "\n",
            "e: 129 loss: 1.83449 train_err: 69.900 train_acc: 30.100 test_top1: 78.390 test_loss 5.27904 test_acc: 21.610\n",
            "\n",
            "e: 130 loss: 1.82894 train_err: 69.844 train_acc: 30.156 test_top1: 79.120 test_loss 10.43134 test_acc: 20.880\n",
            "\n",
            "e: 131 loss: 1.82466 train_err: 69.602 train_acc: 30.398 test_top1: 90.000 test_loss 116.60033 test_acc: 10.000\n",
            "\n",
            "e: 132 loss: 1.82956 train_err: 69.960 train_acc: 30.040 test_top1: 86.640 test_loss 4.12178 test_acc: 13.360\n",
            "\n",
            "e: 133 loss: 1.82699 train_err: 69.790 train_acc: 30.210 test_top1: 73.630 test_loss 2.05525 test_acc: 26.370\n",
            "\n",
            "e: 134 loss: 1.82679 train_err: 70.262 train_acc: 29.738 test_top1: 79.080 test_loss 2.04521 test_acc: 20.920\n",
            "\n",
            "e: 135 loss: 1.82531 train_err: 69.854 train_acc: 30.146 test_top1: 90.120 test_loss 24.56638 test_acc: 9.880\n",
            "\n",
            "e: 136 loss: 1.82950 train_err: 69.776 train_acc: 30.224 test_top1: 78.130 test_loss 2.00177 test_acc: 21.870\n",
            "\n",
            "e: 137 loss: 1.83033 train_err: 69.694 train_acc: 30.306 test_top1: 90.000 test_loss 26.22502 test_acc: 10.000\n",
            "\n",
            "e: 138 loss: 1.81849 train_err: 69.210 train_acc: 30.790 test_top1: 80.440 test_loss 3.12151 test_acc: 19.560\n",
            "\n",
            "e: 139 loss: 1.82531 train_err: 69.866 train_acc: 30.134 test_top1: 82.730 test_loss 2.42387 test_acc: 17.270\n",
            "\n",
            "e: 140 loss: 1.82431 train_err: 69.960 train_acc: 30.040 test_top1: 88.900 test_loss 4.55498 test_acc: 11.100\n",
            "\n",
            "e: 141 loss: 1.82921 train_err: 69.906 train_acc: 30.094 test_top1: 81.680 test_loss 2.76619 test_acc: 18.320\n",
            "\n",
            "e: 142 loss: 1.83282 train_err: 70.248 train_acc: 29.752 test_top1: 81.010 test_loss 3.01675 test_acc: 18.990\n",
            "\n",
            "e: 143 loss: 1.82438 train_err: 69.616 train_acc: 30.384 test_top1: 90.000 test_loss 25.15169 test_acc: 10.000\n",
            "\n",
            "e: 144 loss: 1.82141 train_err: 69.490 train_acc: 30.510 test_top1: 83.230 test_loss 3.04159 test_acc: 16.770\n",
            "\n",
            "e: 145 loss: 1.81431 train_err: 69.652 train_acc: 30.348 test_top1: 79.950 test_loss 2.34840 test_acc: 20.050\n",
            "\n",
            "e: 146 loss: 1.82940 train_err: 69.954 train_acc: 30.046 test_top1: 87.570 test_loss 7.67393 test_acc: 12.430\n",
            "\n",
            "e: 147 loss: 1.82451 train_err: 69.502 train_acc: 30.498 test_top1: 85.120 test_loss 2.73920 test_acc: 14.880\n",
            "\n",
            "e: 148 loss: 1.82977 train_err: 69.692 train_acc: 30.308 test_top1: 83.860 test_loss 4.16083 test_acc: 16.140\n",
            "\n",
            "e: 149 loss: 1.82493 train_err: 69.902 train_acc: 30.098 test_top1: 85.170 test_loss 4.96956 test_acc: 14.830\n",
            "\n",
            "e: 150 loss: 1.82226 train_err: 69.712 train_acc: 30.288 test_top1: 90.570 test_loss 8.39766 test_acc: 9.430\n",
            "\n",
            "e: 151 loss: 1.70910 train_err: 64.322 train_acc: 35.678 test_top1: 65.740 test_loss 1.73677 test_acc: 34.260\n",
            "\n",
            "e: 152 loss: 1.65898 train_err: 62.034 train_acc: 37.966 test_top1: 60.010 test_loss 1.60756 test_acc: 39.990\n",
            "\n",
            "e: 153 loss: 1.61458 train_err: 59.856 train_acc: 40.144 test_top1: 58.290 test_loss 1.60124 test_acc: 41.710\n",
            "\n",
            "e: 154 loss: 1.58233 train_err: 58.458 train_acc: 41.542 test_top1: 59.350 test_loss 1.59635 test_acc: 40.650\n",
            "\n",
            "e: 155 loss: 1.55325 train_err: 57.490 train_acc: 42.510 test_top1: 59.720 test_loss 1.62793 test_acc: 40.280\n",
            "\n",
            "e: 156 loss: 1.52108 train_err: 56.348 train_acc: 43.652 test_top1: 54.880 test_loss 1.48752 test_acc: 45.120\n",
            "\n",
            "e: 157 loss: 1.49498 train_err: 55.034 train_acc: 44.966 test_top1: 54.880 test_loss 1.45899 test_acc: 45.120\n",
            "\n",
            "e: 158 loss: 1.46955 train_err: 53.976 train_acc: 46.024 test_top1: 59.080 test_loss 1.63759 test_acc: 40.920\n",
            "\n",
            "e: 159 loss: 1.44867 train_err: 53.250 train_acc: 46.750 test_top1: 61.860 test_loss 1.73687 test_acc: 38.140\n",
            "\n",
            "e: 160 loss: 1.42798 train_err: 52.644 train_acc: 47.356 test_top1: 63.560 test_loss 1.83055 test_acc: 36.440\n",
            "\n",
            "e: 161 loss: 1.41231 train_err: 51.728 train_acc: 48.272 test_top1: 55.390 test_loss 1.48747 test_acc: 44.610\n",
            "\n",
            "e: 162 loss: 1.39544 train_err: 50.996 train_acc: 49.004 test_top1: 59.090 test_loss 1.73161 test_acc: 40.910\n",
            "\n",
            "e: 163 loss: 1.37329 train_err: 50.098 train_acc: 49.902 test_top1: 81.470 test_loss 4.17989 test_acc: 18.530\n",
            "\n",
            "e: 164 loss: 1.32527 train_err: 48.250 train_acc: 51.750 test_top1: 58.740 test_loss 1.82475 test_acc: 41.260\n",
            "\n",
            "e: 165 loss: 1.27387 train_err: 46.428 train_acc: 53.572 test_top1: 67.890 test_loss 2.15945 test_acc: 32.110\n",
            "\n",
            "e: 166 loss: 1.25076 train_err: 45.472 train_acc: 54.528 test_top1: 46.250 test_loss 1.27439 test_acc: 53.750\n",
            "\n",
            "e: 167 loss: 1.23298 train_err: 44.540 train_acc: 55.460 test_top1: 52.490 test_loss 1.48055 test_acc: 47.510\n",
            "\n",
            "e: 168 loss: 1.22076 train_err: 43.982 train_acc: 56.018 test_top1: 64.930 test_loss 2.09113 test_acc: 35.070\n",
            "\n",
            "e: 169 loss: 1.20366 train_err: 43.198 train_acc: 56.802 test_top1: 49.210 test_loss 1.48209 test_acc: 50.790\n",
            "\n",
            "e: 170 loss: 1.18539 train_err: 42.890 train_acc: 57.110 test_top1: 50.810 test_loss 1.51725 test_acc: 49.190\n",
            "\n",
            "e: 171 loss: 1.17814 train_err: 42.702 train_acc: 57.298 test_top1: 52.300 test_loss 1.56907 test_acc: 47.700\n",
            "\n",
            "e: 172 loss: 1.17439 train_err: 42.414 train_acc: 57.586 test_top1: 50.950 test_loss 1.45117 test_acc: 49.050\n",
            "\n",
            "e: 173 loss: 1.15564 train_err: 41.620 train_acc: 58.380 test_top1: 46.840 test_loss 1.33864 test_acc: 53.160\n",
            "\n",
            "e: 174 loss: 1.14743 train_err: 41.342 train_acc: 58.658 test_top1: 49.070 test_loss 1.33614 test_acc: 50.930\n",
            "\n",
            "e: 175 loss: 1.13907 train_err: 41.126 train_acc: 58.874 test_top1: 40.050 test_loss 1.11565 test_acc: 59.950\n",
            "\n",
            "e: 176 loss: 1.12699 train_err: 40.596 train_acc: 59.404 test_top1: 45.570 test_loss 1.26616 test_acc: 54.430\n",
            "\n",
            "e: 177 loss: 1.11951 train_err: 40.338 train_acc: 59.662 test_top1: 47.810 test_loss 1.36802 test_acc: 52.190\n",
            "\n",
            "e: 178 loss: 1.11363 train_err: 40.156 train_acc: 59.844 test_top1: 56.210 test_loss 1.68336 test_acc: 43.790\n",
            "\n",
            "e: 179 loss: 1.10301 train_err: 39.670 train_acc: 60.330 test_top1: 58.330 test_loss 1.81428 test_acc: 41.670\n",
            "\n",
            "e: 180 loss: 1.09419 train_err: 39.394 train_acc: 60.606 test_top1: 46.750 test_loss 1.35473 test_acc: 53.250\n",
            "\n",
            "e: 181 loss: 1.08428 train_err: 38.710 train_acc: 61.290 test_top1: 45.120 test_loss 1.27503 test_acc: 54.880\n",
            "\n",
            "e: 182 loss: 1.08095 train_err: 38.714 train_acc: 61.286 test_top1: 60.060 test_loss 2.09927 test_acc: 39.940\n",
            "\n",
            "e: 183 loss: 1.07375 train_err: 38.670 train_acc: 61.330 test_top1: 50.780 test_loss 1.46169 test_acc: 49.220\n",
            "\n",
            "e: 184 loss: 1.06741 train_err: 38.370 train_acc: 61.630 test_top1: 44.950 test_loss 1.26010 test_acc: 55.050\n",
            "\n",
            "e: 185 loss: 1.06850 train_err: 38.520 train_acc: 61.480 test_top1: 44.710 test_loss 1.40938 test_acc: 55.290\n",
            "\n",
            "e: 186 loss: 1.06355 train_err: 38.266 train_acc: 61.734 test_top1: 48.300 test_loss 1.40634 test_acc: 51.700\n",
            "\n",
            "e: 187 loss: 1.05638 train_err: 37.806 train_acc: 62.194 test_top1: 44.010 test_loss 1.23363 test_acc: 55.990\n",
            "\n",
            "e: 188 loss: 1.05448 train_err: 37.748 train_acc: 62.252 test_top1: 46.790 test_loss 1.35432 test_acc: 53.210\n",
            "\n",
            "e: 189 loss: 1.05143 train_err: 37.780 train_acc: 62.220 test_top1: 47.360 test_loss 1.31950 test_acc: 52.640\n",
            "\n",
            "e: 190 loss: 1.04289 train_err: 37.220 train_acc: 62.780 test_top1: 70.720 test_loss 2.56406 test_acc: 29.280\n",
            "\n",
            "e: 191 loss: 1.04320 train_err: 37.410 train_acc: 62.590 test_top1: 55.820 test_loss 2.48955 test_acc: 44.180\n",
            "\n",
            "e: 192 loss: 1.03729 train_err: 36.866 train_acc: 63.134 test_top1: 37.570 test_loss 1.06415 test_acc: 62.430\n",
            "\n",
            "e: 193 loss: 1.03316 train_err: 36.882 train_acc: 63.118 test_top1: 37.810 test_loss 1.11472 test_acc: 62.190\n",
            "\n",
            "e: 194 loss: 1.03073 train_err: 36.804 train_acc: 63.196 test_top1: 46.210 test_loss 1.33444 test_acc: 53.790\n",
            "\n",
            "e: 195 loss: 1.02249 train_err: 36.596 train_acc: 63.404 test_top1: 38.740 test_loss 1.09228 test_acc: 61.260\n",
            "\n",
            "e: 196 loss: 1.02645 train_err: 36.768 train_acc: 63.232 test_top1: 43.420 test_loss 1.28744 test_acc: 56.580\n",
            "\n",
            "e: 197 loss: 1.01886 train_err: 36.358 train_acc: 63.642 test_top1: 63.460 test_loss 2.40069 test_acc: 36.540\n",
            "\n",
            "e: 198 loss: 1.01371 train_err: 36.042 train_acc: 63.958 test_top1: 42.340 test_loss 1.26238 test_acc: 57.660\n",
            "\n",
            "e: 199 loss: 1.01564 train_err: 36.204 train_acc: 63.796 test_top1: 46.210 test_loss 1.33872 test_acc: 53.790\n",
            "\n",
            "e: 200 loss: 1.01289 train_err: 36.034 train_acc: 63.966 test_top1: 54.600 test_loss 2.10991 test_acc: 45.400\n",
            "\n",
            "e: 201 loss: 1.00827 train_err: 35.962 train_acc: 64.038 test_top1: 39.080 test_loss 1.09918 test_acc: 60.920\n",
            "\n",
            "e: 202 loss: 1.00684 train_err: 35.964 train_acc: 64.036 test_top1: 62.860 test_loss 2.90420 test_acc: 37.140\n",
            "\n",
            "e: 203 loss: 1.00899 train_err: 35.810 train_acc: 64.190 test_top1: 58.480 test_loss 1.99552 test_acc: 41.520\n",
            "\n",
            "e: 204 loss: 1.00101 train_err: 35.522 train_acc: 64.478 test_top1: 51.540 test_loss 1.50503 test_acc: 48.460\n",
            "\n",
            "e: 205 loss: 1.00029 train_err: 35.516 train_acc: 64.484 test_top1: 53.840 test_loss 1.79497 test_acc: 46.160\n",
            "\n",
            "e: 206 loss: 0.99998 train_err: 35.584 train_acc: 64.416 test_top1: 50.000 test_loss 1.82682 test_acc: 50.000\n",
            "\n",
            "e: 207 loss: 1.00263 train_err: 35.574 train_acc: 64.426 test_top1: 53.520 test_loss 1.57418 test_acc: 46.480\n",
            "\n",
            "e: 208 loss: 0.99899 train_err: 35.422 train_acc: 64.578 test_top1: 40.510 test_loss 1.15857 test_acc: 59.490\n",
            "\n",
            "e: 209 loss: 0.99145 train_err: 35.082 train_acc: 64.918 test_top1: 48.730 test_loss 1.49978 test_acc: 51.270\n",
            "\n",
            "e: 210 loss: 0.99823 train_err: 35.676 train_acc: 64.324 test_top1: 39.390 test_loss 1.10451 test_acc: 60.610\n",
            "\n",
            "e: 211 loss: 0.98972 train_err: 35.072 train_acc: 64.928 test_top1: 49.400 test_loss 1.66852 test_acc: 50.600\n",
            "\n",
            "e: 212 loss: 0.98923 train_err: 35.184 train_acc: 64.816 test_top1: 50.330 test_loss 1.57318 test_acc: 49.670\n",
            "\n",
            "e: 213 loss: 0.98742 train_err: 35.198 train_acc: 64.802 test_top1: 35.770 test_loss 1.00847 test_acc: 64.230\n",
            "\n",
            "e: 214 loss: 0.98790 train_err: 35.058 train_acc: 64.942 test_top1: 45.670 test_loss 1.58552 test_acc: 54.330\n",
            "\n",
            "e: 215 loss: 0.98461 train_err: 34.810 train_acc: 65.190 test_top1: 43.270 test_loss 1.27843 test_acc: 56.730\n",
            "\n",
            "e: 216 loss: 0.98557 train_err: 34.992 train_acc: 65.008 test_top1: 42.350 test_loss 1.27474 test_acc: 57.650\n",
            "\n",
            "e: 217 loss: 0.98287 train_err: 34.774 train_acc: 65.226 test_top1: 46.590 test_loss 1.33623 test_acc: 53.410\n",
            "\n",
            "e: 218 loss: 0.98041 train_err: 34.884 train_acc: 65.116 test_top1: 48.030 test_loss 1.47011 test_acc: 51.970\n",
            "\n",
            "e: 219 loss: 0.98024 train_err: 34.636 train_acc: 65.364 test_top1: 47.460 test_loss 1.36807 test_acc: 52.540\n",
            "\n",
            "e: 220 loss: 0.98229 train_err: 34.796 train_acc: 65.204 test_top1: 44.740 test_loss 1.35988 test_acc: 55.260\n",
            "\n",
            "e: 221 loss: 0.98122 train_err: 34.654 train_acc: 65.346 test_top1: 41.850 test_loss 1.18120 test_acc: 58.150\n",
            "\n",
            "e: 222 loss: 0.98113 train_err: 34.968 train_acc: 65.032 test_top1: 48.580 test_loss 1.64620 test_acc: 51.420\n",
            "\n",
            "e: 223 loss: 0.98236 train_err: 34.956 train_acc: 65.044 test_top1: 45.310 test_loss 1.36261 test_acc: 54.690\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot(EPOCH_LIST, TRAIN_ACC_LIST, TEST_ACC_LIST)"
      ],
      "metadata": {
        "id": "Uj8LAmwaQ5tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#change model\n",
        "model = 'vgg16'\n"
      ],
      "metadata": {
        "id": "x1500RdczgsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_val = 'sgd'\n",
        "EPOCH_LIST, TRAIN_ACC_LIST, TEST_ACC_LIST = main(lr_val, start_epoch)"
      ],
      "metadata": {
        "id": "5qot1eej1e8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(EPOCH_LIST, TRAIN_ACC_LIST, TEST_ACC_LIST)"
      ],
      "metadata": {
        "id": "42MHyuosRrR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_val = 'adam'\n",
        "EPOCH_LIST, TRAIN_ACC_LIST, TEST_ACC_LIST = main(lr_val, start_epoch)"
      ],
      "metadata": {
        "id": "TOzexxkQ1ixH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(EPOCH_LIST, TRAIN_ACC_LIST, TEST_ACC_LIST)"
      ],
      "metadata": {
        "id": "ajmoboHORsrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CHANGE TO CIPHER100\n",
        "\n",
        "datatype = 'cipher100'"
      ],
      "metadata": {
        "id": "7Afoz3pZbBjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#change model\n",
        "model = 'resnet18'"
      ],
      "metadata": {
        "id": "XH8gi3zSbPoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_val = 'sgd'\n",
        "EPOCH_LIST, TRAIN_ACC_LIST, TEST_ACC_LIST = main(lr_val, start_epoch)"
      ],
      "metadata": {
        "id": "OzkQd_99bZ9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(EPOCH_LIST, TRAIN_ACC_LIST, TEST_ACC_LIST)"
      ],
      "metadata": {
        "id": "25shpGVgRvSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_val = 'adam'\n",
        "EPOCH_LIST, TRAIN_ACC_LIST, TEST_ACC_LIST = main(lr_val, start_epoch)"
      ],
      "metadata": {
        "id": "TnwCOp6Dbuyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(EPOCH_LIST, TRAIN_ACC_LIST, TEST_ACC_LIST)"
      ],
      "metadata": {
        "id": "f4ef5eBbRwgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#change model\n",
        "model = 'vgg16'\n"
      ],
      "metadata": {
        "id": "O845zWjzby-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_val = 'sgd'\n",
        "EPOCH_LIST, TRAIN_ACC_LIST, TEST_ACC_LIST = main(lr_val, start_epoch)"
      ],
      "metadata": {
        "id": "1KBXjeAMbztM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(EPOCH_LIST, TRAIN_ACC_LIST, TEST_ACC_LIST)"
      ],
      "metadata": {
        "id": "22crTT_rRx5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_val = 'adam'\n",
        "EPOCH_LIST, TRAIN_ACC_LIST, TEST_ACC_LIST = main(lr_val, start_epoch)"
      ],
      "metadata": {
        "id": "rDXxGBfHb4f4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(EPOCH_LIST, TRAIN_ACC_LIST, TEST_ACC_LIST)"
      ],
      "metadata": {
        "id": "Pv_04-tzRytT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}