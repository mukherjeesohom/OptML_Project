{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a1dc2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b572f75c",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "488aafb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicUnit(nn.Module):\n",
    "    def __init__(self, channels: int, dropout: float):\n",
    "        super(BasicUnit, self).__init__()\n",
    "        self.block = nn.Sequential(OrderedDict([\n",
    "            (\"0_normalization\", nn.BatchNorm2d(channels)),\n",
    "            (\"1_activation\", nn.ReLU(inplace=True)),\n",
    "            (\"2_convolution\", nn.Conv2d(channels, channels, (3, 3), stride=1, padding=1, bias=False)),\n",
    "            (\"3_normalization\", nn.BatchNorm2d(channels)),\n",
    "            (\"4_activation\", nn.ReLU(inplace=True)),\n",
    "            (\"5_dropout\", nn.Dropout(dropout, inplace=True)),\n",
    "            (\"6_convolution\", nn.Conv2d(channels, channels, (3, 3), stride=1, padding=1, bias=False)),\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58cb0a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownsampleUnit(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int, dropout: float):\n",
    "        super(DownsampleUnit, self).__init__()\n",
    "        self.norm_act = nn.Sequential(OrderedDict([\n",
    "            (\"0_normalization\", nn.BatchNorm2d(in_channels)),\n",
    "            (\"1_activation\", nn.ReLU(inplace=True)),\n",
    "        ]))\n",
    "        self.block = nn.Sequential(OrderedDict([\n",
    "            (\"0_convolution\", nn.Conv2d(in_channels, out_channels, (3, 3), stride=stride, padding=1, bias=False)),\n",
    "            (\"1_normalization\", nn.BatchNorm2d(out_channels)),\n",
    "            (\"2_activation\", nn.ReLU(inplace=True)),\n",
    "            (\"3_dropout\", nn.Dropout(dropout, inplace=True)),\n",
    "            (\"4_convolution\", nn.Conv2d(out_channels, out_channels, (3, 3), stride=1, padding=1, bias=False)),\n",
    "        ]))\n",
    "        self.downsample = nn.Conv2d(in_channels, out_channels, (1, 1), stride=stride, padding=0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm_act(x)\n",
    "        return self.block(x) + self.downsample(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60b3e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int, depth: int, dropout: float):\n",
    "        super(Block, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            DownsampleUnit(in_channels, out_channels, stride, dropout),\n",
    "            *(BasicUnit(out_channels, dropout) for _ in range(depth))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b883b1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, depth: int, width_factor: int, dropout: float, in_channels: int, labels: int):\n",
    "        super(WideResNet, self).__init__()\n",
    "\n",
    "        self.filters = [16, 1 * 16 * width_factor, 2 * 16 * width_factor, 4 * 16 * width_factor]\n",
    "        self.block_depth = (depth - 4) // (3 * 2)\n",
    "\n",
    "        self.f = nn.Sequential(OrderedDict([\n",
    "            (\"0_convolution\", nn.Conv2d(in_channels, self.filters[0], (3, 3), stride=1, padding=1, bias=False)),\n",
    "            (\"1_block\", Block(self.filters[0], self.filters[1], 1, self.block_depth, dropout)),\n",
    "            (\"2_block\", Block(self.filters[1], self.filters[2], 2, self.block_depth, dropout)),\n",
    "            (\"3_block\", Block(self.filters[2], self.filters[3], 2, self.block_depth, dropout)),\n",
    "            (\"4_normalization\", nn.BatchNorm2d(self.filters[3])),\n",
    "            (\"5_activation\", nn.ReLU(inplace=True)),\n",
    "            (\"6_pooling\", nn.AvgPool2d(kernel_size=8)),\n",
    "            (\"7_flattening\", nn.Flatten()),\n",
    "            (\"8_classification\", nn.Linear(in_features=self.filters[3], out_features=labels)),\n",
    "        ]))\n",
    "\n",
    "        self._initialize()\n",
    "\n",
    "    def _initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight.data, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.zero_()\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318e17c1",
   "metadata": {},
   "source": [
    "### Smooth entropy done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "510467ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_crossentropy(pred, gold, smoothing=0.1):\n",
    "    n_class = pred.size(1)\n",
    "\n",
    "    one_hot = torch.full_like(pred, fill_value=smoothing / (n_class - 1))\n",
    "    one_hot.scatter_(dim=1, index=gold.unsqueeze(1), value=1.0 - smoothing)\n",
    "    log_prob = F.log_softmax(pred, dim=1)\n",
    "\n",
    "    return F.kl_div(input=log_prob, target=one_hot, reduction='none').sum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e0400a",
   "metadata": {},
   "source": [
    "### Data Preparation done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fca0ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cutout:\n",
    "    def __init__(self, size=16, p=0.5):\n",
    "        self.size = size\n",
    "        self.half_size = size // 2\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if torch.rand([1]).item() > self.p:\n",
    "            return image\n",
    "\n",
    "        left = torch.randint(-self.half_size, image.size(1) - self.half_size, [1]).item()\n",
    "        top = torch.randint(-self.half_size, image.size(2) - self.half_size, [1]).item()\n",
    "        right = min(image.size(1), left + self.size)\n",
    "        bottom = min(image.size(2), top + self.size)\n",
    "\n",
    "        image[:, max(0, left): right, max(0, top): bottom] = 0\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70b9926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar:\n",
    "    def __init__(self, batch_size, threads):\n",
    "        mean, std = self._get_statistics()\n",
    "\n",
    "        train_transform = transforms.Compose([\n",
    "            torchvision.transforms.RandomCrop(size=(32, 32), padding=4),\n",
    "            torchvision.transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std),\n",
    "            Cutout()\n",
    "        ])\n",
    "\n",
    "        test_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ])\n",
    "\n",
    "        train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "        test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "        self.train = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=threads)\n",
    "        self.test = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=threads)\n",
    "\n",
    "        self.classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    def _get_statistics(self):\n",
    "        train_set = torchvision.datasets.CIFAR10(root='./cifar', train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "        data = torch.cat([d[0] for d in DataLoader(train_set)])\n",
    "        return data.mean(dim=[0, 2, 3]), data.std(dim=[0, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60717adc",
   "metadata": {},
   "source": [
    "### Step LR and Initialize and Bypass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d32d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepLR:\n",
    "    def __init__(self, optimizer, learning_rate: float, total_epochs: int):\n",
    "        self.optimizer = optimizer\n",
    "        self.total_epochs = total_epochs\n",
    "        self.base = learning_rate\n",
    "\n",
    "    def __call__(self, epoch):\n",
    "        if epoch < self.total_epochs * 3/10:\n",
    "            lr = self.base\n",
    "        elif epoch < self.total_epochs * 6/10:\n",
    "            lr = self.base * 0.2\n",
    "        elif epoch < self.total_epochs * 8/10:\n",
    "            lr = self.base * 0.2 ** 2\n",
    "        else:\n",
    "            lr = self.base * 0.2 ** 3\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "    def lr(self) -> float:\n",
    "        return self.optimizer.param_groups[0][\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dce91b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(seed: int):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0c65cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disable_running_stats(model):\n",
    "    def _disable(module):\n",
    "        if isinstance(module, nn.BatchNorm2d):\n",
    "            module.backup_momentum = module.momentum\n",
    "            module.momentum = 0\n",
    "\n",
    "    model.apply(_disable)\n",
    "\n",
    "def enable_running_stats(model):\n",
    "    def _enable(module):\n",
    "        if isinstance(module, nn.BatchNorm2d) and hasattr(module, \"backup_momentum\"):\n",
    "            module.momentum = module.backup_momentum\n",
    "\n",
    "    model.apply(_enable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0623d5",
   "metadata": {},
   "source": [
    "### SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcba7a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                self.state[p][\"old_p\"] = p.data.clone()\n",
    "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        super().load_state_dict(state_dict)\n",
    "        self.base_optimizer.param_groups = self.param_groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158db4bf",
   "metadata": {},
   "source": [
    "### Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a4a56a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "adaptive = True\n",
    "batch_size = 128\n",
    "depth = 16\n",
    "dropout = 0.0\n",
    "label_smoothing = 0.1\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "threads = 2\n",
    "rho = 2.0\n",
    "weight_decay = 0.0005\n",
    "width_factor = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a968a5cf",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "feedca1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "initialize(seed=42)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01cee922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = Cifar(batch_size, threads)\n",
    "model = WideResNet(depth, width_factor, dropout, in_channels=3, labels=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdd06c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_optimizer = torch.optim.Adam\n",
    "optimizer = SAM(\n",
    "    model.parameters(), \n",
    "    base_optimizer, \n",
    "    rho=rho, \n",
    "    adaptive=adaptive, \n",
    "    lr=learning_rate, \n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "scheduler = StepLR(optimizer, learning_rate, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7288d9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    for batch in dataset.train:\n",
    "        inputs, targets = (b.to(device) for b in batch)\n",
    "\n",
    "        # first forward-backward step\n",
    "        enable_running_stats(model)\n",
    "        predictions = model(inputs)\n",
    "        loss = smooth_crossentropy(predictions, targets, smoothing=label_smoothing)\n",
    "        loss.mean().backward()\n",
    "        optimizer.first_step(zero_grad=True)\n",
    "\n",
    "        # second forward-backward step\n",
    "        disable_running_stats(model)\n",
    "        smooth_crossentropy(model(inputs), targets, smoothing=label_smoothing).mean().backward()\n",
    "        optimizer.second_step(zero_grad=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            correct = torch.argmax(predictions.data, 1) == targets\n",
    "            log(model, loss.cpu(), correct.cpu(), scheduler.lr())\n",
    "            scheduler(epoch)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataset.test:\n",
    "            inputs, targets = (b.to(device) for b in batch)\n",
    "\n",
    "            predictions = model(inputs)\n",
    "            loss = smooth_crossentropy(predictions, targets)\n",
    "            correct = torch.argmax(predictions, 1) == targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed483c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
