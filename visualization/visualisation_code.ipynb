{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OPTML.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aPzqnIz-T95u"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.parallel\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#set variables\n",
        "\n",
        "#training options\n",
        "batch_size = 128\n",
        "lr_val = 0.1\n",
        "start_epoch = 1\n",
        "lr_decay = 0.1\n",
        "optimizer_val = 'sgd' #adam \n",
        "weight_decay = 0.0005\n",
        "momentum = 0.9\n",
        "epochs = 2\n",
        "ngpu = 1\n",
        "rand_seed = 0\n",
        "resume_model = ''\n",
        "resume_opt = ''\n",
        "\n",
        "#model parameters\n",
        "model = 'resnet18'\n",
        "loss_name = 'crossentropy'\n",
        "raw_data = False\n",
        "noaug = False\n",
        "label_corrupt_prob = 0.0\n",
        "trainloader_val = ''\n",
        "testloader_val = ''\n",
        "idx = 0\n",
        "\n",
        "datatype = 'cipher10'"
      ],
      "metadata": {
        "id": "Pr17sTxJUDYO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "# print('Current devices: ' + str(torch.cuda.current_device()))\n",
        "# print('Device count: ' + str(torch.cuda.device_count()))"
      ],
      "metadata": {
        "id": "pMcTERNYUHCU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_loaders(trainloader, testloader, datatype):\n",
        "    if trainloader and testloader:\n",
        "        assert os.path.exists(trainloader), 'trainloader does not exist'\n",
        "        assert os.path.exists(testloader), 'testloader does not exist'\n",
        "        trainloader = torch.load(trainloader)\n",
        "        testloader = torch.load(testloader)\n",
        "        return trainloader, testloader\n",
        "\n",
        "    normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
        "                                     std=[x/255.0 for x in [63.0, 62.1, 66.7]])\n",
        "\n",
        "    if raw_data:\n",
        "        transform_train = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "    else:\n",
        "        if not noaug:\n",
        "            # with data augmentation\n",
        "            transform_train = transforms.Compose([\n",
        "                transforms.RandomCrop(32, padding=4),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                normalize,\n",
        "            ])\n",
        "        else:\n",
        "            # no data agumentation\n",
        "            transform_train = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                normalize,\n",
        "            ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "\n",
        "\n",
        "    kwargs = {'num_workers': 2, 'pin_memory': True} if ngpu else {}\n",
        "\n",
        "    if datatype=='cipher10':\n",
        "      trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True,\n",
        "                                              transform=transform_train)\n",
        "      testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True,\n",
        "                                            transform=transform_test)\n",
        "\n",
        "      trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                                shuffle=True, **kwargs)\n",
        "      testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                              shuffle=False, **kwargs)\n",
        "    else:\n",
        "      trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True,\n",
        "                                              transform=transform_train)\n",
        "      testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True,\n",
        "                                            transform=transform_test)\n",
        "\n",
        "      trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                                shuffle=True, **kwargs)\n",
        "      testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                              shuffle=False, **kwargs)\n",
        "\n",
        "    return trainloader, testloader"
      ],
      "metadata": {
        "id": "ATjEXsU-YllX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#basicblock and resnet class\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1   = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2   = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=100):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1  = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1    = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ImageNet models\n",
        "def ResNet18():\n",
        "  if datatype=='cipher10':\n",
        "    return ResNet(BasicBlock, [2,2,2,2], 10)\n",
        "  \n",
        "  return ResNet(BasicBlock, [2,2,2,2], 100)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vh3saaDpa__G"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cfg = {\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name, num):\n",
        "        super(VGG, self).__init__()\n",
        "        self.input_size = 32\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.n_maps = cfg[vgg_name][-2]\n",
        "        self.fc = self._make_fc_layers()\n",
        "        self.classifier = nn.Linear(self.n_maps, num)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_fc_layers(self):\n",
        "        layers = []\n",
        "        layers += [nn.Linear(self.n_maps*self.input_size*self.input_size, self.n_maps),\n",
        "                   nn.BatchNorm1d(self.n_maps),\n",
        "                   nn.ReLU(inplace=True)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n",
        "                self.input_size = self.input_size // 2\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def VGG16():\n",
        "  if datatype=='cipher10':\n",
        "    return VGG('VGG16', 10)\n",
        "  return VGG('VGG16', 100)\n",
        "\n"
      ],
      "metadata": {
        "id": "zRg93FvGwQJv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# map between model name and function\n",
        "models = {\n",
        "    'resnet18'              : ResNet18,\n",
        "    'vgg16'                  : VGG16,\n",
        "}\n"
      ],
      "metadata": {
        "id": "x_HKn81FbSF-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load(model_name, model_file=None, data_parallel=False):\n",
        "    net = models[model_name]()\n",
        "    if data_parallel: # the model is saved in data paralle mode\n",
        "        net = torch.nn.DataParallel(net)\n",
        "\n",
        "    if model_file:\n",
        "        assert os.path.exists(model_file), model_file + \" does not exist.\"\n",
        "        stored = torch.load(model_file, map_location=lambda storage, loc: storage)\n",
        "        if 'state_dict' in stored.keys():\n",
        "            net.load_state_dict(stored['state_dict'])\n",
        "        else:\n",
        "            net.load_state_dict(stored)\n",
        "\n",
        "    if data_parallel: # convert the model back to the single GPU version\n",
        "        net = net.module\n",
        "\n",
        "    net.eval()\n",
        "    return net"
      ],
      "metadata": {
        "id": "ERa0lTljbWgh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize params; only for first training\n",
        "\n",
        "def init_params(net):\n",
        "    for m in net.modules():\n",
        "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "            init.kaiming_normal_(m.weight, mode='fan_in')\n",
        "            if m.bias is not None:\n",
        "                init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            init.constant_(m.weight, 1)\n",
        "            init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            init.normal_(m.weight, std=1e-3)\n",
        "            if m.bias is not None:\n",
        "                init.constant_(m.bias, 0)"
      ],
      "metadata": {
        "id": "5m1EOEMfbk3a"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "def train(trainloader, net, criterion, optimizer, use_cuda=True):\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    if isinstance(criterion, nn.CrossEntropyLoss):\n",
        "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "            batch_size = inputs.size(0)\n",
        "            total += batch_size\n",
        "            if use_cuda:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            inputs, targets = Variable(inputs), Variable(targets)\n",
        "            \n",
        "            \n",
        "            if optimizer_val=='sgd' or optimizer_val=='adam':\n",
        "              outputs = net(inputs)\n",
        "              loss = criterion(outputs, targets)\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "              \n",
        "           \n",
        "\n",
        "            train_loss += loss.item()*batch_size\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct += predicted.eq(targets.data).cpu().sum().item()\n",
        "\n",
        "    \n",
        "\n",
        "    return train_loss/total, 100 - 100.*correct/total,  100.*correct/total\n"
      ],
      "metadata": {
        "id": "roR8NY8rdmeT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(testloader, net, criterion, use_cuda=True):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    if isinstance(criterion, nn.CrossEntropyLoss):\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            batch_size = inputs.size(0)\n",
        "            total += batch_size\n",
        "\n",
        "            if use_cuda:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            inputs, targets = Variable(inputs), Variable(targets)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            test_loss += loss.item()*batch_size\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct += predicted.eq(targets.data).cpu().sum().item()\n",
        "\n",
        "  \n",
        "\n",
        "    return test_loss/total, 100 - 100.*correct/total,  100.*correct/total"
      ],
      "metadata": {
        "id": "qbywkJM8doQX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(lr_val, start_epoch):\n",
        "  random.seed(rand_seed)\n",
        "  np.random.seed(rand_seed)\n",
        "  torch.manual_seed(rand_seed)\n",
        "\n",
        "  if use_cuda:\n",
        "    torch.cuda.manual_seed_all(rand_seed)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "  \n",
        "  \n",
        "\n",
        "  trainloader, testloader = get_data_loaders(trainloader_val, testloader_val, datatype)\n",
        "\n",
        "\n",
        "\n",
        "  # Model\n",
        "  if resume_model:\n",
        "      # Load checkpoint.\n",
        "      print('==> Resuming from checkpoint..')\n",
        "      checkpoint = torch.load(resume_model)\n",
        "      net = load(model)\n",
        "     \n",
        "\n",
        "      net.load_state_dict(checkpoint['state_dict'])\n",
        "      start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "  else:\n",
        "      net = load(model)    \n",
        "\n",
        "  \n",
        "  if ngpu > 1:\n",
        "        net = torch.nn.DataParallel(net)\n",
        "\n",
        "  \n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "  if use_cuda:\n",
        "        net.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "\n",
        "  \n",
        "\n",
        "  # Optimizer\n",
        "  if optimizer_val == 'sgd':\n",
        "      optimizer = optim.SGD(net.parameters(), lr=lr_val, momentum=momentum, weight_decay=weight_decay, nesterov=True)\n",
        "  elif optimizer_val=='adam':\n",
        "      optimizer = optim.Adam(net.parameters(), lr=lr_val, weight_decay=weight_decay)\n",
        "  \n",
        "\n",
        "\n",
        "  \n",
        "  if resume_opt:\n",
        "    checkpoint_opt = torch.load(resume_opt)\n",
        "    optimizer.load_state_dict(checkpoint_opt['optimizer'])\n",
        "\n",
        "  \n",
        "\n",
        "  # record the performance of initial model\n",
        "  if not resume_model:\n",
        "      train_loss, train_err, train_acc = test(trainloader, net, criterion, use_cuda)\n",
        "      test_loss, test_err, test_acc = test(testloader, net, criterion, use_cuda)\n",
        "      status = 'e: %d loss: %.5f train_err: %.3f train_acc: %.3f test_top1: %.3f test_loss %.5f test_acc: %.3f\\n' % (0, train_loss, train_err, train_acc, test_err, test_loss, test_acc)\n",
        "      print(status)\n",
        "   \n",
        "  \n",
        "  for epoch in range(start_epoch, epochs + 1):\n",
        "        loss, train_err, train_acc = train(trainloader, net, criterion, optimizer, use_cuda)\n",
        "        test_loss, test_err, test_acc = test(testloader, net, criterion, use_cuda)\n",
        "        status = 'e: %d loss: %.5f train_err: %.3f train_acc: %.3f test_top1: %.3f test_loss %.5f test_acc: %.3f\\n' % (epoch, loss, train_err, train_acc, test_err, test_loss, test_acc)\n",
        "        print(status)\n",
        "       \n",
        "\n",
        "        if int(epoch) == 150 or int(epoch) == 225 or int(epoch) == 275:\n",
        "            lr_val *= lr_decay\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] *= lr_decay\n",
        "\n",
        "  torch.save(net.state_dict(),'model_'+model+\"_\"+datatype+\"_\"+optimizer_val+\"_\" '.pt')\n",
        "\n"
      ],
      "metadata": {
        "id": "G1UiKXuUUJy7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(lr_val, start_epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfAWcZYEUqG2",
        "outputId": "b968bfc4-0b71-490b-b430-8d13bd29fe11"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
            "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e: 0 loss: 2.30289 train_err: 90.000 train_acc: 10.000 test_top1: 90.000 test_loss 2.30289 test_acc: 10.000\n",
            "\n",
            "e: 1 loss: 2.30289 train_err: 84.058 train_acc: 15.942 test_top1: 90.030 test_loss 3.48685 test_acc: 9.970\n",
            "\n",
            "e: 2 loss: 2.30289 train_err: 80.634 train_acc: 19.366 test_top1: 77.240 test_loss 1.90435 test_acc: 22.760\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_val = 'adam'\n",
        "main(lr_val, start_epoch)"
      ],
      "metadata": {
        "id": "bXihymYJg8Il",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd0dc801-b1fb-4825-b3ba-906e35802bf5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "e: 0 loss: 2.30271 train_err: 89.748 train_acc: 10.252 test_top1: 89.390 test_loss 2.30278 test_acc: 10.610\n",
            "\n",
            "e: 0 loss: 2.30271 train_err: 76.188 train_acc: 23.812 test_top1: 73.170 test_loss 1.93808 test_acc: 26.830\n",
            "\n",
            "e: 0 loss: 2.30271 train_err: 70.852 train_acc: 29.148 test_top1: 72.820 test_loss 1.85577 test_acc: 27.180\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#change model\n",
        "model = 'vgg16'\n"
      ],
      "metadata": {
        "id": "x1500RdczgsD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_val = 'sgd'\n",
        "main(lr_val, start_epoch)"
      ],
      "metadata": {
        "id": "5qot1eej1e8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_val = 'adam'\n",
        "main(lr_val, start_epoch)"
      ],
      "metadata": {
        "id": "TOzexxkQ1ixH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CHANGE TO CIPHER100\n",
        "\n",
        "datatype = 'cipher100'"
      ],
      "metadata": {
        "id": "7Afoz3pZbBjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#change model\n",
        "model = 'resnet18'"
      ],
      "metadata": {
        "id": "XH8gi3zSbPoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_val = 'sgd'\n",
        "main(lr_val, start_epoch)"
      ],
      "metadata": {
        "id": "OzkQd_99bZ9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_val = 'adam'\n",
        "main(lr_val, start_epoch)"
      ],
      "metadata": {
        "id": "TnwCOp6Dbuyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#change model\n",
        "model = 'vgg16'\n"
      ],
      "metadata": {
        "id": "O845zWjzby-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_val = 'sgd'\n",
        "main(lr_val, start_epoch)"
      ],
      "metadata": {
        "id": "1KBXjeAMbztM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_val = 'adam'\n",
        "main(lr_val, start_epoch)"
      ],
      "metadata": {
        "id": "rDXxGBfHb4f4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}