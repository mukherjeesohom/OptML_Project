{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OPTML.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPzqnIz-T95u"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.parallel\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#set variables\n",
        "\n",
        "#training options\n",
        "batch_size = 128\n",
        "lr_val = 0.1\n",
        "start_epoch = 1\n",
        "lr_decay = 0.1\n",
        "optimizer_val = 'sgd' #adam | lbfgs\n",
        "weight_decay = 0.0005\n",
        "momentum = 0.9\n",
        "epochs = 2\n",
        "save = 'trained_nets'\n",
        "save_epoch = 10\n",
        "ngpu = 1\n",
        "rand_seed = 0\n",
        "resume_model = ''\n",
        "resume_opt = ''\n",
        "save_folder = ''\n",
        "\n",
        "#model parameters\n",
        "model = 'resnet18'\n",
        "loss_name = 'crossentropy'\n",
        "raw_data = False\n",
        "noaug = False\n",
        "label_corrupt_prob = 0.0\n",
        "trainloader_val = ''\n",
        "testloader_val = ''\n",
        "idx = 0"
      ],
      "metadata": {
        "id": "Pr17sTxJUDYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "# print('Current devices: ' + str(torch.cuda.current_device()))\n",
        "# print('Device count: ' + str(torch.cuda.device_count()))"
      ],
      "metadata": {
        "id": "pMcTERNYUHCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def name_save_folder():\n",
        "    save_folder = 'model' + '_' + str(optimizer_val) + '_lr=' + str(lr_val)\n",
        "    if lr_decay != 0.1:\n",
        "        save_folder += '_lr_decay=' + str(lr_decay)\n",
        "    save_folder += '_bs=' + str(batch_size)\n",
        "    save_folder += '_wd=' + str(weight_decay)\n",
        "    save_folder += '_mom=' + str(momentum)\n",
        "    save_folder += '_save_epoch=' + str(save_epoch)\n",
        "    if loss_name != 'crossentropy':\n",
        "        save_folder += '_loss=' + str(loss_name)\n",
        "    if noaug:\n",
        "        save_folder += '_noaug'\n",
        "    if raw_data:\n",
        "        save_folder += '_rawdata'\n",
        "    if label_corrupt_prob > 0:\n",
        "        save_folder += '_randlabel=' + str(label_corrupt_prob)\n",
        "    if ngpu > 1:\n",
        "        save_folder += '_ngpu=' + str(ngpu)\n",
        "    if idx:\n",
        "        save_folder += '_idx=' + str(idx)\n",
        "\n",
        "    return save_folder"
      ],
      "metadata": {
        "id": "AtBq5thGVwRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_loaders(trainloader, testloader):\n",
        "    if trainloader and testloader:\n",
        "        assert os.path.exists(trainloader), 'trainloader does not exist'\n",
        "        assert os.path.exists(testloader), 'testloader does not exist'\n",
        "        trainloader = torch.load(trainloader)\n",
        "        testloader = torch.load(testloader)\n",
        "        return trainloader, testloader\n",
        "\n",
        "    normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
        "                                     std=[x/255.0 for x in [63.0, 62.1, 66.7]])\n",
        "\n",
        "    if raw_data:\n",
        "        transform_train = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "    else:\n",
        "        if not noaug:\n",
        "            # with data augmentation\n",
        "            transform_train = transforms.Compose([\n",
        "                transforms.RandomCrop(32, padding=4),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                normalize,\n",
        "            ])\n",
        "        else:\n",
        "            # no data agumentation\n",
        "            transform_train = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                normalize,\n",
        "            ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "\n",
        "\n",
        "    kwargs = {'num_workers': 2, 'pin_memory': True} if ngpu else {}\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True,\n",
        "                                            transform=transform_train)\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True,\n",
        "                                           transform=transform_test)\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                              shuffle=True, **kwargs)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                             shuffle=False, **kwargs)\n",
        "\n",
        "    return trainloader, testloader"
      ],
      "metadata": {
        "id": "ATjEXsU-YllX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#basicblock and resnet class\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1   = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2   = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1  = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1    = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ImageNet models\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2,2,2,2])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vh3saaDpa__G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# map between model name and function\n",
        "models = {\n",
        "    'resnet18'              : ResNet18\n",
        "}\n"
      ],
      "metadata": {
        "id": "x_HKn81FbSF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load(model_name, model_file=None, data_parallel=False):\n",
        "    net = models[model_name]()\n",
        "    if data_parallel: # the model is saved in data paralle mode\n",
        "        net = torch.nn.DataParallel(net)\n",
        "\n",
        "    if model_file:\n",
        "        assert os.path.exists(model_file), model_file + \" does not exist.\"\n",
        "        stored = torch.load(model_file, map_location=lambda storage, loc: storage)\n",
        "        if 'state_dict' in stored.keys():\n",
        "            net.load_state_dict(stored['state_dict'])\n",
        "        else:\n",
        "            net.load_state_dict(stored)\n",
        "\n",
        "    if data_parallel: # convert the model back to the single GPU version\n",
        "        net = net.module\n",
        "\n",
        "    net.eval()\n",
        "    return net"
      ],
      "metadata": {
        "id": "ERa0lTljbWgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize params; only for first training\n",
        "\n",
        "def init_params(net):\n",
        "    for m in net.modules():\n",
        "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "            init.kaiming_normal_(m.weight, mode='fan_in')\n",
        "            if m.bias is not None:\n",
        "                init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            init.constant_(m.weight, 1)\n",
        "            init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            init.normal_(m.weight, std=1e-3)\n",
        "            if m.bias is not None:\n",
        "                init.constant_(m.bias, 0)"
      ],
      "metadata": {
        "id": "5m1EOEMfbk3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "def train(trainloader, net, criterion, optimizer, use_cuda=True):\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    if isinstance(criterion, nn.CrossEntropyLoss):\n",
        "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "            batch_size = inputs.size(0)\n",
        "            total += batch_size\n",
        "            if use_cuda:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            inputs, targets = Variable(inputs), Variable(targets)\n",
        "            \n",
        "            \n",
        "            if optimizer_val=='sgd' or optimizer_val=='adam':\n",
        "              outputs = net(inputs)\n",
        "              loss = criterion(outputs, targets)\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "              \n",
        "            else:\n",
        "              def closure():\n",
        "                optimizer.zero_grad()\n",
        "                outputs = net(inputs)\n",
        "                loss = criterion(outputs, one_hot_targets)\n",
        "                loss.backward()\n",
        "                return loss    \n",
        "                          \n",
        "              optimizer.step(closure)\n",
        "\n",
        "            train_loss += loss.item()*batch_size\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct += predicted.eq(targets.data).cpu().sum().item()\n",
        "\n",
        "    elif isinstance(criterion, nn.MSELoss):\n",
        "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "            batch_size = inputs.size(0)\n",
        "            total += batch_size\n",
        "            one_hot_targets = torch.FloatTensor(batch_size, 10).zero_()\n",
        "            one_hot_targets = one_hot_targets.scatter_(1, targets.view(batch_size, 1), 1.0)\n",
        "            one_hot_targets = one_hot_targets.float()\n",
        "            if use_cuda:\n",
        "                inputs, one_hot_targets = inputs.cuda(), one_hot_targets.cuda()\n",
        "            inputs, one_hot_targets = Variable(inputs), Variable(one_hot_targets)\n",
        "            \n",
        "            \n",
        "            if optimizer_val=='sgd' or optimizer_val=='adam':\n",
        "              outputs = F.softmax(net(inputs))\n",
        "              loss = criterion(outputs, one_hot_targets)\n",
        "\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "            else:\n",
        "              def closure():\n",
        "                optimizer.zero_grad()\n",
        "                outputs = F.softmax(net(inputs))\n",
        "                loss = criterion(outputs, one_hot_targets)\n",
        "                loss.backward()\n",
        "                return loss    \n",
        "              \n",
        "              loss = optimizer.step(closure)\n",
        "              outputs = F.softmax(net(inputs))\n",
        "            train_loss += loss.item()*batch_size\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct += predicted.cpu().eq(targets).cpu().sum().item()\n",
        "\n",
        "    return train_loss/total, 100 - 100.*correct/total\n"
      ],
      "metadata": {
        "id": "roR8NY8rdmeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(testloader, net, criterion, use_cuda=True):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    if isinstance(criterion, nn.CrossEntropyLoss):\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            batch_size = inputs.size(0)\n",
        "            total += batch_size\n",
        "\n",
        "            if use_cuda:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            inputs, targets = Variable(inputs), Variable(targets)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            test_loss += loss.item()*batch_size\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct += predicted.eq(targets.data).cpu().sum().item()\n",
        "\n",
        "    elif isinstance(criterion, nn.MSELoss):\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            batch_size = inputs.size(0)\n",
        "            total += batch_size\n",
        "\n",
        "            one_hot_targets = torch.FloatTensor(batch_size, 10).zero_()\n",
        "            one_hot_targets = one_hot_targets.scatter_(1, targets.view(batch_size, 1), 1.0)\n",
        "            one_hot_targets = one_hot_targets.float()\n",
        "            if use_cuda:\n",
        "                inputs, one_hot_targets = inputs.cuda(), one_hot_targets.cuda()\n",
        "            inputs, one_hot_targets = Variable(inputs), Variable(one_hot_targets)\n",
        "            outputs = F.softmax(net(inputs))\n",
        "            loss = criterion(outputs, one_hot_targets)\n",
        "            test_loss += loss.item()*batch_size\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct += predicted.cpu().eq(targets).cpu().sum().item()\n",
        "\n",
        "    return test_loss/total, 100 - 100.*correct/total"
      ],
      "metadata": {
        "id": "qbywkJM8doQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(lr_val, start_epoch):\n",
        "  random.seed(rand_seed)\n",
        "  np.random.seed(rand_seed)\n",
        "  torch.manual_seed(rand_seed)\n",
        "\n",
        "  if use_cuda:\n",
        "    torch.cuda.manual_seed_all(rand_seed)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "  \n",
        "  if not os.path.isdir(save):\n",
        "    os.mkdir(save)\n",
        "\n",
        "  save_folder = name_save_folder()\n",
        "\n",
        "  if not os.path.exists('trained_nets/' + save_folder):\n",
        "    os.makedirs('trained_nets/' + save_folder)\n",
        "\n",
        "  f = open('trained_nets/' + save_folder + '/log.out', 'a')\n",
        "\n",
        "  trainloader, testloader = get_data_loaders(trainloader_val, testloader_val)\n",
        "\n",
        "\n",
        "  if label_corrupt_prob and not resume_model:\n",
        "        torch.save(trainloader, 'trained_nets/' + save_folder + '/trainloader.dat')\n",
        "        torch.save(testloader, 'trained_nets/' + save_folder + '/testloader.dat')\n",
        "\n",
        "  \n",
        "\n",
        "  # Model\n",
        "  if resume_model:\n",
        "      # Load checkpoint.\n",
        "      print('==> Resuming from checkpoint..')\n",
        "      checkpoint = torch.load(resume_model)\n",
        "      if optimizer_val=='sgd':  #check if its first time; since first time we use sgd\n",
        "        net = load(model)\n",
        "      else:\n",
        "        net = pytorch_model\n",
        "\n",
        "      net.load_state_dict(checkpoint['state_dict'])\n",
        "      start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "  else:\n",
        "      if optimizer_val=='sgd':  #check if its first time; since first time we use sgd\n",
        "        net = load(model)\n",
        "      else:\n",
        "        net = pytorch_model\n",
        "    \n",
        "    \n",
        "\n",
        "  \n",
        "  if ngpu > 1:\n",
        "        net = torch.nn.DataParallel(net)\n",
        "\n",
        "  \n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "  if loss_name == 'crossentropy':\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "\n",
        "  if use_cuda:\n",
        "        net.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "\n",
        "  \n",
        "\n",
        "  # Optimizer\n",
        "  if optimizer_val == 'sgd':\n",
        "      optimizer = optim.SGD(net.parameters(), lr=lr_val, momentum=momentum, weight_decay=weight_decay, nesterov=True)\n",
        "  elif optimizer_val=='adam':\n",
        "      optimizer = optim.Adam(net.parameters(), lr=lr_val, weight_decay=weight_decay)\n",
        "  else:\n",
        "      optimizer = optim.LBFGS(net.parameters(), lr=lr_val)\n",
        "\n",
        "\n",
        "  \n",
        "  if resume_opt:\n",
        "    checkpoint_opt = torch.load(resume_opt)\n",
        "    optimizer.load_state_dict(checkpoint_opt['optimizer'])\n",
        "\n",
        "  \n",
        "\n",
        "  # record the performance of initial model\n",
        "  if not resume_model:\n",
        "      train_loss, train_err = test(trainloader, net, criterion, use_cuda)\n",
        "      test_loss, test_err = test(testloader, net, criterion, use_cuda)\n",
        "      status = 'e: %d loss: %.5f train_err: %.3f test_top1: %.3f test_loss %.5f \\n' % (0, train_loss, train_err, test_err, test_loss)\n",
        "      print(status)\n",
        "      f.write(status)\n",
        "      \n",
        "      state = {\n",
        "          'acc': 100 - test_err,\n",
        "          'epoch': 0,\n",
        "          'state_dict': net.module.state_dict() if ngpu > 1 else net.state_dict()\n",
        "      }\n",
        "      opt_state = {\n",
        "          'optimizer': optimizer.state_dict()\n",
        "      }\n",
        "      torch.save(state, 'trained_nets/' + save_folder + '/model_0.t7')\n",
        "      torch.save(opt_state, 'trained_nets/' + save_folder + '/opt_state_0.t7')\n",
        "\n",
        "      \n",
        "  \n",
        "  for epoch in range(start_epoch, epochs + 1):\n",
        "        loss, train_err = train(trainloader, net, criterion, optimizer, use_cuda)\n",
        "        test_loss, test_err = test(testloader, net, criterion, use_cuda)\n",
        "        status = 'e: %d loss: %.5f train_err: %.3f test_top1: %.3f test_loss %.5f \\n' % (epoch, loss, train_err, test_err, test_loss)\n",
        "        print(status)\n",
        "        f.write(status)\n",
        "        \n",
        "\n",
        "        # Save checkpoint.\n",
        "        acc = 100 - test_err\n",
        "        if epoch == 1 or epoch % save_epoch == 0 or epoch == 150:\n",
        "            state = {\n",
        "                'acc': acc,\n",
        "                'epoch': epoch,\n",
        "                'state_dict': net.module.state_dict() if ngpu > 1 else net.state_dict(),\n",
        "            }\n",
        "            opt_state = {\n",
        "                'optimizer': optimizer.state_dict()\n",
        "            }\n",
        "            torch.save(state, 'trained_nets/' + save_folder + '/model_' + str(epoch) + '.t7')\n",
        "            torch.save(opt_state, 'trained_nets/' + save_folder + '/opt_state_' + str(epoch) + '.t7')\n",
        "\n",
        "            \n",
        "\n",
        "        if int(epoch) == 150 or int(epoch) == 225 or int(epoch) == 275:\n",
        "            lr_val *= lr_decay\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] *= lr_decay\n",
        "\n",
        "  torch.save(net.state_dict(),'model_'+optimizer_val+\"_\" '.pt')\n",
        "\n",
        "  f.close()"
      ],
      "metadata": {
        "id": "G1UiKXuUUJy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(lr_val, start_epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfAWcZYEUqG2",
        "outputId": "a2a7ec48-f2c7-46a0-8e98-e44a0919c296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e: 0 loss: 0.09000 train_err: 89.748 test_top1: 89.390 test_loss 0.09000 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e: 1 loss: 0.11360 train_err: 88.188 test_top1: 88.130 test_loss 0.15573 \n",
            "\n",
            "e: 2 loss: 0.17713 train_err: 89.716 test_top1: 88.910 test_loss 0.17781 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def setmodel(optimizer_val):\n",
        "  pytorch_model =  ResNet18()\n",
        "\n",
        "  #/content/model_sgd_.pt\n",
        "  pytorch_model.load_state_dict(torch.load('/content/model_'+optimizer_val+'_.pt'))\n",
        "  pytorch_model.eval()\n",
        "\n",
        "  return pytorch_model\n",
        "\n"
      ],
      "metadata": {
        "id": "bQX5_ruVUx47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pytorch_model = setmodel('sgd')\n",
        "optimizer_val = 'adam'\n",
        "main(lr_val, start_epoch)"
      ],
      "metadata": {
        "id": "bXihymYJg8Il",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23e7cd41-7427-417c-9142-c7f74e68106d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e: 0 loss: 0.17854 train_err: 89.270 test_top1: 88.910 test_loss 0.17781 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e: 1 loss: 0.18003 train_err: 90.020 test_top1: 90.000 test_loss 0.18000 \n",
            "\n",
            "e: 2 loss: 0.18000 train_err: 90.000 test_top1: 90.000 test_loss 0.18000 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pytorch_model = setmodel('adam')\n",
        "optimizer_val = 'LBFGS'\n",
        "main(lr_val, start_epoch)"
      ],
      "metadata": {
        "id": "auKmbX8f3VtS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48275c2e-53f7-404e-e380-687825f07971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e: 0 loss: 0.18000 train_err: 90.000 test_top1: 90.000 test_loss 0.18000 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:65: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e: 1 loss: 0.18000 train_err: 90.000 test_top1: 90.000 test_loss 0.18000 \n",
            "\n",
            "e: 2 loss: 0.18000 train_err: 90.000 test_top1: 90.000 test_loss 0.18000 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "edCbf6Bplo9d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}